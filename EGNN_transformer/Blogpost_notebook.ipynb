{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09204ced",
   "metadata": {},
   "source": [
    "### Making wrapper for QM9 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cfee9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26278/2732476931.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.file_address)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train-set, task: homo, source: ./QM9_data/QM9_data.pt, length: 100000\n",
      "MINIBATCH\n",
      "Graph(num_nodes=575, num_edges=10254,\n",
      "      ndata_schemes={'x': Scheme(shape=(3,), dtype=torch.float32), 'f': Scheme(shape=(6, 1), dtype=torch.float32)}\n",
      "      edata_schemes={'d': Scheme(shape=(3,), dtype=torch.float32), 'w': Scheme(shape=(5,), dtype=torch.float32)})\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# QM9 -> dgl\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import dgl\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from scipy.constants import physical_constants\n",
    "\n",
    "hartree2eV = physical_constants['hartree-electron volt relationship'][0]\n",
    "DTYPE = np.float32\n",
    "DTYPE_INT = np.int32\n",
    "\n",
    "class QM9Dataset(Dataset):\n",
    "    \"\"\"QM9 dataset.\"\"\"\n",
    "    num_bonds = 4\n",
    "    atom_feature_size = 6 \n",
    "    input_keys = ['mol_id', 'num_atoms', 'num_bonds', 'x', 'one_hot', \n",
    "                  'atomic_numbers', 'edge']\n",
    "    unit_conversion = {'mu': 1.0,\n",
    "                       'alpha': 1.0,\n",
    "                       'homo': hartree2eV,\n",
    "                       'lumo': hartree2eV,\n",
    "                       'gap': hartree2eV, \n",
    "                       'r2': 1.0, \n",
    "                       'zpve': hartree2eV, \n",
    "                       'u0': hartree2eV, \n",
    "                       'u298': hartree2eV, \n",
    "                       'h298': hartree2eV,\n",
    "                       'g298': hartree2eV,\n",
    "                       'cv': 1.0} \n",
    "\n",
    "    def __init__(self, file_address: str, task: str, mode: str='train', \n",
    "            transform=None, fully_connected: bool=False): \n",
    "        \"\"\"Create a dataset object\n",
    "\n",
    "        Args:\n",
    "            file_address: path to data\n",
    "            task: target task [\"homo\", ...]\n",
    "            mode: [train/val/test] mode\n",
    "            transform: data augmentation functions\n",
    "            fully_connected: return a fully connected graph\n",
    "        \"\"\"\n",
    "        self.file_address = file_address\n",
    "        self.task = task\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.fully_connected = fully_connected\n",
    "\n",
    "        # Encode and extra bond type for fully connected graphs\n",
    "        self.num_bonds += fully_connected\n",
    "\n",
    "        self.load_data()\n",
    "        self.len = len(self.targets)\n",
    "        print(f\"Loaded {mode}-set, task: {task}, source: {self.file_address}, length: {len(self)}\")\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    \n",
    "    def load_data(self):\n",
    "        # Load dict and select train/valid/test split\n",
    "        data = torch.load(self.file_address)\n",
    "        data = data[self.mode]\n",
    "    \n",
    "        # Filter out the inputs\n",
    "        self.inputs = {key: data[key] for key in self.input_keys}\n",
    "\n",
    "        # Filter out the targets and population stats\n",
    "        self.targets = data[self.task]\n",
    "\n",
    "        # TODO: use the training stats unlike the other papers\n",
    "        self.mean = np.mean(self.targets)\n",
    "        self.std = np.std(self.targets)\n",
    "\n",
    "\n",
    "    def get_target(self, idx, normalize=True):\n",
    "        target = self.targets[idx]\n",
    "        if normalize:\n",
    "            target = (target - self.mean) / self.std\n",
    "        return target\n",
    "\n",
    "\n",
    "    def norm2units(self, x, denormalize=True, center=True):\n",
    "        # Convert from normalized to QM9 representation\n",
    "        if denormalize:\n",
    "            x = x * self.std\n",
    "            # Add the mean: not necessary for error computations\n",
    "            if not center:\n",
    "                x += self.mean\n",
    "        x = self.unit_conversion[self.task] * x\n",
    "        return x\n",
    "\n",
    "\n",
    "    def to_one_hot(self, data, num_classes):\n",
    "        one_hot = np.zeros(list(data.shape) + [num_classes])\n",
    "        one_hot[np.arange(len(data)),data] = 1\n",
    "        return one_hot\n",
    "\n",
    "\n",
    "    def _get_adjacency(self, n_atoms):\n",
    "        # Adjust adjacency structure\n",
    "        seq = np.arange(n_atoms)\n",
    "        src = seq[:,None] * np.ones((1,n_atoms), dtype=np.int32)\n",
    "        dst = src.T\n",
    "        ## Remove diagonals and reshape\n",
    "        src[seq, seq] = -1\n",
    "        dst[seq, seq] = -1\n",
    "        src, dst = src.reshape(-1), dst.reshape(-1)\n",
    "        src, dst = src[src > -1], dst[dst > -1]\n",
    "            \n",
    "        return src, dst\n",
    "\n",
    "\n",
    "    def get(self, key, idx):\n",
    "        return self.inputs[key][idx]\n",
    "\n",
    "\n",
    "    def connect_fully(self, edges, num_atoms):\n",
    "        \"\"\"Convert to a fully connected graph\"\"\"\n",
    "        # Initialize all edges: no self-edges\n",
    "        adjacency = {}\n",
    "        for i in range(num_atoms):\n",
    "            for j in range(num_atoms):\n",
    "                if i != j:\n",
    "                    # assigning new type of connection if originally not connected\n",
    "                    adjacency[(i, j)] = self.num_bonds - 1 \n",
    "\n",
    "        # Add bonded edges\n",
    "        for idx in range(edges.shape[0]):\n",
    "            adjacency[(edges[idx,0], edges[idx,1])] = edges[idx,2]\n",
    "            adjacency[(edges[idx,1], edges[idx,0])] = edges[idx,2]\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        src = []\n",
    "        dst = []\n",
    "        w = []\n",
    "        for edge, weight in adjacency.items():\n",
    "            src.append(edge[0])\n",
    "            dst.append(edge[1])\n",
    "            w.append(weight)\n",
    "\n",
    "        return np.array(src), np.array(dst), np.array(w)\n",
    "\n",
    "\n",
    "    def connect_partially(self, edge):\n",
    "        src = np.concatenate([edge[:,0], edge[:,1]])\n",
    "        dst = np.concatenate([edge[:,1], edge[:,0]])\n",
    "        w = np.concatenate([edge[:,2], edge[:,2]])\n",
    "        return src, dst, w\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load node features\n",
    "        num_atoms = self.get('num_atoms', idx) # number of atoms\n",
    "        x = self.get('x', idx)[:num_atoms].astype(DTYPE) # coordinates of atoms\n",
    "        one_hot = self.get('one_hot', idx)[:num_atoms].astype(DTYPE)\n",
    "        atomic_numbers = self.get('atomic_numbers', idx)[:num_atoms].astype(DTYPE)\n",
    "\n",
    "        # Load edge features\n",
    "        num_bonds = self.get('num_bonds', idx)\n",
    "        edge = self.get('edge', idx)[:num_bonds]\n",
    "        edge = np.asarray(edge, dtype=DTYPE_INT)\n",
    "\n",
    "        # Load target\n",
    "        y = self.get_target(idx, normalize=True).astype(DTYPE)\n",
    "        y = np.array([y])\n",
    "\n",
    "        # Augmentation on the coordinates\n",
    "        if self.transform:\n",
    "            x = self.transform(x).astype(DTYPE)\n",
    "\n",
    "        # Create nodes\n",
    "        if self.fully_connected:\n",
    "            src, dst, w = self.connect_fully(edge, num_atoms)\n",
    "        else:\n",
    "            src, dst, w = self.connect_partially(edge)\n",
    "        w = self.to_one_hot(w, self.num_bonds).astype(DTYPE)\n",
    "\n",
    "        # Create graph\n",
    "        G = dgl.DGLGraph((src, dst))\n",
    "\n",
    "        # Add node features to graph\n",
    "        G.ndata['x'] = torch.tensor(x) #[num_atoms,3]\n",
    "        G.ndata['f'] = torch.tensor(np.concatenate([one_hot, atomic_numbers], -1)[...,None]) #[num_atoms,6,1]\n",
    "\n",
    "        # Add edge features to graph\n",
    "        G.edata['d'] = torch.tensor(x[dst] - x[src]) #[num_atoms,3]\n",
    "        G.edata['w'] = torch.tensor(w) #[num_atoms,4]\n",
    "\n",
    "        return G, y\n",
    "\n",
    "\n",
    "\n",
    "def collate(samples):\n",
    "    graphs, y = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.tensor(y)\n",
    "\n",
    "dataset = QM9Dataset('./QM9_data/QM9_data.pt', \"homo\", mode='train', fully_connected=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "\n",
    "iter_dataloader = iter(dataloader) # so I can use next\n",
    "for i in range(1):\n",
    "    data = next(iter_dataloader)\n",
    "    print(\"MINIBATCH\")\n",
    "    print(data[0]) \n",
    "    print(data[1].shape) # batch size -> connected graph of size batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38a1438-fd3c-4b5f-9980-54b1fa451dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa590534-6c05-4802-a2a5-1aa984e5aac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0537c631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9,\n",
       "        9, 9]),\n",
       " array([1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 3, 4,\n",
       "        5, 6, 7, 8, 9, 0, 1, 2, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 5, 6, 7, 8,\n",
       "        9, 0, 1, 2, 3, 4, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 7, 8, 9, 0, 1, 2,\n",
       "        3, 4, 5, 6, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 9, 0, 1, 2, 3, 4, 5, 6,\n",
       "        7, 8]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_adjacency(n_atoms):\n",
    "    # Adjust adjacency structure\n",
    "    seq = np.arange(n_atoms)\n",
    "    src = seq[:,None] * np.ones((1,n_atoms), dtype=np.int32)\n",
    "    dst = src.T\n",
    "    ## Remove diagonals and reshape\n",
    "    src[seq, seq] = -1\n",
    "    dst[seq, seq] = -1\n",
    "    src, dst = src.reshape(-1), dst.reshape(-1)\n",
    "    src, dst = src[src > -1], dst[dst > -1]\n",
    "\n",
    "    return src, dst\n",
    "\n",
    "_get_adjacency(10) # from src to dst fully connected in this case, no self connections\n",
    "\n",
    "# all the connections are represented in the form\n",
    "# src[0]   src[1]   ...\n",
    "# dst[0]   dst[1]   ...\n",
    "# w[0]     w[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa0e36a",
   "metadata": {},
   "source": [
    "### DGL example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74c45e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1268,  1.4886, -0.3711],\n",
      "        [-0.0493,  0.0360,  0.0684],\n",
      "        [-1.3388, -0.5307, -0.0172],\n",
      "        ...,\n",
      "        [-1.7919, -2.3873,  1.5352],\n",
      "        [ 1.7500, -0.5402,  1.3364],\n",
      "        [ 0.4925,  0.5138,  2.0511]])\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "torch.Size([10254, 3])\n",
      "tensor([[[0.2344, 0.6642, 0.0072, 0.3524],\n",
      "         [0.1158, 0.8405, 0.0173, 0.1057],\n",
      "         [0.5976, 0.4465, 0.7831, 0.4323]],\n",
      "\n",
      "        [[0.9599, 0.0646, 0.9775, 0.0375],\n",
      "         [0.3635, 0.0576, 0.7881, 0.6512],\n",
      "         [0.0792, 0.4827, 0.6080, 0.1550]],\n",
      "\n",
      "        [[0.1088, 0.4255, 0.4270, 0.9816],\n",
      "         [0.5244, 0.2549, 0.4082, 0.5101],\n",
      "         [0.6430, 0.9398, 0.0098, 0.3039]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.1396, 0.4124, 0.5984, 0.5986],\n",
      "         [0.6577, 0.0709, 0.0222, 0.6742],\n",
      "         [0.7836, 0.3820, 0.1077, 0.4304]],\n",
      "\n",
      "        [[0.1292, 0.6439, 0.4525, 0.1526],\n",
      "         [0.3316, 0.1640, 0.2102, 0.6573],\n",
      "         [0.8755, 0.9895, 0.6988, 0.4968]],\n",
      "\n",
      "        [[0.7993, 0.1091, 0.3453, 0.6253],\n",
      "         [0.7267, 0.3242, 0.7604, 0.4147],\n",
      "         [0.8924, 0.3386, 0.9587, 0.3529]]], dtype=torch.float64)\n",
      "torch.Size([10254, 1])\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "\n",
    "\n",
    "dataset = data\n",
    "\n",
    "\n",
    "\n",
    "# initialize dgl graph\n",
    "G = dataset[0]\n",
    "\n",
    "ntype = 'x'\n",
    "# ndata is a dict\n",
    "# retrieve data from all nodes labels ntype (position example)\n",
    "print(G.ndata[ntype]) \n",
    "\n",
    "d = 5\n",
    "G.ndata[f'out{d}'] = torch.tensor(np.zeros((len(G.ndata['x']), 5)))\n",
    "# retrieve output features of type d from node data\n",
    "print(G.ndata[f'out{d}']) \n",
    "\n",
    "\n",
    "etype = 'd'\n",
    "# retrive data from all edges labeled etype\n",
    "print(G.edata[etype].shape)\n",
    "\n",
    "di = 'd'\n",
    "do = 'w'\n",
    "G.edata[f'({di},{do})'] = torch.tensor(np.random.rand(G.edata[etype].shape[0], 3, 4))\n",
    "# retrive edge kernels that transform from type di to type di\n",
    "print(G.edata[f'({di},{do})'])\n",
    "\n",
    "e = 'd' # edge feature (distance)\n",
    "v = 'x' # node features (cartesian vector)\n",
    "m = 'm' # output message on the edge\n",
    "# calling built in dgl fubction e_dot_v that computes a message on\n",
    "# edge by performing element-wise dot between features of e and v\n",
    "# and stores it as edge message labeled 'm'\n",
    "f = fn.e_dot_v(e, v, m)\n",
    "\n",
    "# applies the function f to update the features of the edges with function\n",
    "G.apply_edges(f)\n",
    "\n",
    "print(G.edata['m'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfed102",
   "metadata": {},
   "source": [
    "### Fibers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "871dcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.utils_profiling import * # load before other local modules\n",
    "try:\n",
    "    profile\n",
    "except NameError:\n",
    "    def profile(func):\n",
    "        return func\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "class Fiber(object):\n",
    "    \"\"\"A Handy Data Structure for Fibers\"\"\"\n",
    "    def __init__(self, num_degrees: int=None, num_channels: int=None,\n",
    "                 structure: List[Tuple[int,int]]=None, dictionary=None):\n",
    "        \"\"\"\n",
    "        define fiber structure; use one num_degrees & num_channels OR structure\n",
    "        OR dictionary\n",
    "\n",
    "        :param num_degrees: degrees will be [0, ..., num_degrees-1]\n",
    "        :param num_channels: number of channels, same for each degree\n",
    "        :param structure: e.g. [(32, 0),(16, 1),(16,2)]\n",
    "        :param dictionary: e.g. {0:32, 1:16, 2:16}\n",
    "        \n",
    "        Structure in the form: List[(Tuple[int, int])]. In particular Features[(num_channels, feature_degree)]\n",
    "        \"\"\"\n",
    "        \n",
    "        if structure:\n",
    "            self.structure = structure\n",
    "        elif dictionary:\n",
    "            self.structure = [(dictionary[o], o) for o in sorted(dictionary.keys())]\n",
    "        else:\n",
    "            self.structure = [(num_channels, i) for i in range(num_degrees)]\n",
    "\n",
    "            \n",
    "        # assigning to dict format and computing cummulative variables\n",
    "        self.multiplicities, self.degrees = zip(*self.structure)\n",
    "        self.max_degree = max(self.degrees)\n",
    "        self.min_degree = min(self.degrees)\n",
    "        self.structure_dict = {k: v for v, k in self.structure}\n",
    "        self.dict = self.structure_dict\n",
    "        self.n_features = np.sum([i[0] * (2*i[1]+1) for i in self.structure])\n",
    "\n",
    "        \n",
    "        # Mapping to vec() case. f = [...] with starting ind saved in feature ind dict\n",
    "        # feature_ind = {degree: starting ind}\n",
    "        self.feature_indices = {}\n",
    "        idx = 0\n",
    "        for (num_channels, d) in self.structure:\n",
    "            length = num_channels * (2*d + 1)\n",
    "            self.feature_indices[d] = (idx, idx + length)\n",
    "            idx += length\n",
    "\n",
    "    def copy_me(self, multiplicity: int=None):\n",
    "        s = copy.deepcopy(self.structure)\n",
    "        if multiplicity is not None:\n",
    "            # overwrite multiplicities\n",
    "            s = [(multiplicity, o) for m, o in s]\n",
    "        return Fiber(structure=s)\n",
    "\n",
    "    @staticmethod\n",
    "    def combine(f1, f2):\n",
    "        new_dict = copy.deepcopy(f1.structure_dict)\n",
    "        for k, m in f2.structure_dict.items():\n",
    "            if k in new_dict.keys():\n",
    "                new_dict[k] += m\n",
    "            else:\n",
    "                new_dict[k] = m\n",
    "        structure = [(new_dict[k], k) for k in sorted(new_dict.keys())]\n",
    "        return Fiber(structure=structure)\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_max(f1, f2):\n",
    "        new_dict = copy.deepcopy(f1.structure_dict)\n",
    "        for k, m in f2.structure_dict.items():\n",
    "            if k in new_dict.keys():\n",
    "                new_dict[k] = max(m, new_dict[k])\n",
    "            else:\n",
    "                new_dict[k] = m\n",
    "        structure = [(new_dict[k], k) for k in sorted(new_dict.keys())]\n",
    "        return Fiber(structure=structure)\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_selectively(f1, f2):\n",
    "        # only use orders which occur in fiber f1\n",
    "\n",
    "        new_dict = copy.deepcopy(f1.structure_dict)\n",
    "        for k in f1.degrees:\n",
    "            if k in f2.degrees:\n",
    "                new_dict[k] += f2.structure_dict[k]\n",
    "        structure = [(new_dict[k], k) for k in sorted(new_dict.keys())]\n",
    "        return Fiber(structure=structure)\n",
    "\n",
    "    @staticmethod\n",
    "    def combine_fibers(val1, struc1, val2, struc2):\n",
    "        \"\"\"\n",
    "        combine two fibers\n",
    "\n",
    "        :param val1/2: fiber tensors in dictionary form\n",
    "        :param struc1/2: structure of fiber\n",
    "        :return: fiber tensor in dictionary form\n",
    "        \"\"\"\n",
    "        struc_out = Fiber.combine(struc1, struc2)\n",
    "        val_out = {}\n",
    "        for k in struc_out.degrees:\n",
    "            if k in struc1.degrees:\n",
    "                if k in struc2.degrees:\n",
    "                    val_out[k] = torch.cat([val1[k], val2[k]], -2)\n",
    "                else:\n",
    "                    val_out[k] = val1[k]\n",
    "            else:\n",
    "                val_out[k] = val2[k]\n",
    "                \n",
    "            # number of channels is the second dimenstion from the end I guess\n",
    "            # might look like [tensor_axis = degree, channel axis, tensor-component axis]\n",
    "            assert val_out[k].shape[-2] == struc_out.structure_dict[k]\n",
    "        return val_out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.structure}\"\n",
    "\n",
    "\n",
    "\n",
    "def get_fiber_dict(F, struc, mask=None, return_struc=False):\n",
    "    if mask is None: mask = struc\n",
    "    index = 0\n",
    "    fiber_dict = {}\n",
    "    first_dims = F.shape[:-1]\n",
    "    masked_dict = {}\n",
    "    for o, m in struc.structure_dict.items():\n",
    "        length = m * (2*o + 1)\n",
    "        if o in mask.degrees:\n",
    "            masked_dict[o] = m\n",
    "            fiber_dict[o] = F[...,index:index + length].view(list(first_dims) + [m, 2*o + 1])\n",
    "        index += length\n",
    "    assert F.shape[-1] == index\n",
    "    if return_struc:\n",
    "        return fiber_dict, Fiber(dictionary=masked_dict)\n",
    "    return fiber_dict\n",
    "\n",
    "\n",
    "def get_fiber_tensor(F, struc):\n",
    "    some_entry = tuple(F.values())[0]\n",
    "    first_dims = some_entry.shape[:-2]\n",
    "    res = some_entry.new_empty([*first_dims, struc.n_features])\n",
    "    index = 0\n",
    "    for o, m in struc.structure_dict.items():\n",
    "        length = m * (2*o + 1)\n",
    "        res[..., index: index + length] = F[o].view(*first_dims, length)\n",
    "        index += length\n",
    "    assert index == res.shape[-1]\n",
    "    return res\n",
    "\n",
    "\n",
    "def fiber2tensor(F, structure, squeeze=False):\n",
    "    if squeeze:\n",
    "        fibers = [F[f'{i}'].view(*F[f'{i}'].shape[:-2], -1) for i in structure.degrees]\n",
    "        fibers = torch.cat(fibers, -1)\n",
    "    else:\n",
    "        fibers = [F[f'{i}'].view(*F[f'{i}'].shape[:-2], -1, 1) for i in structure.degrees]\n",
    "        fibers = torch.cat(fibers, -2)\n",
    "    return fibers\n",
    "\n",
    "\n",
    "# Reduce fibers into single tensor cell h (I guess)\n",
    "def fiber2head(F, h, structure, squeeze=False):\n",
    "    if squeeze:\n",
    "        fibers = [F[f'{i}'].view(*F[f'{i}'].shape[:-2], h, -1) for i in structure.degrees]\n",
    "        fibers = torch.cat(fibers, -1)\n",
    "    else:\n",
    "        fibers = [F[f'{i}'].view(*F[f'{i}'].shape[:-2], h, -1, 1) for i in structure.degrees]\n",
    "        fibers = torch.cat(fibers, -2)\n",
    "    return fibers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15238cd4",
   "metadata": {},
   "source": [
    "### Basis transformation matrixes and irreducable representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e44a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cache in files\n",
    "'''\n",
    "from functools import wraps, lru_cache\n",
    "import pickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import fcntl\n",
    "\n",
    "\n",
    "class FileSystemMutex:\n",
    "    '''\n",
    "    Mutual exclusion of different **processes** using the file system\n",
    "    '''\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.handle = None\n",
    "        self.filename = filename\n",
    "\n",
    "    def acquire(self):\n",
    "        '''\n",
    "        Locks the mutex\n",
    "        if it is already locked, it waits (blocking function)\n",
    "        '''\n",
    "        self.handle = open(self.filename, 'w')\n",
    "        fcntl.lockf(self.handle, fcntl.LOCK_EX)\n",
    "        self.handle.write(\"{}\\n\".format(os.getpid()))\n",
    "        self.handle.flush()\n",
    "\n",
    "    def release(self):\n",
    "        '''\n",
    "        Unlock the mutex\n",
    "        '''\n",
    "        if self.handle is None:\n",
    "            raise RuntimeError()\n",
    "        fcntl.lockf(self.handle, fcntl.LOCK_UN)\n",
    "        self.handle.close()\n",
    "        self.handle = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.acquire()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.release()\n",
    "\n",
    "\n",
    "def cached_dirpklgz(dirname, maxsize=128):\n",
    "    '''\n",
    "    Cache a function with a directory\n",
    "\n",
    "    :param dirname: the directory path\n",
    "    :param maxsize: maximum size of the RAM cache (there is no limit for the directory cache)\n",
    "    '''\n",
    "\n",
    "    def decorator(func):\n",
    "        '''\n",
    "        The actual decorator\n",
    "        '''\n",
    "\n",
    "        @lru_cache(maxsize=maxsize)\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            '''\n",
    "            The wrapper of the function\n",
    "            '''\n",
    "            try:\n",
    "                os.makedirs(dirname)\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "\n",
    "            indexfile = os.path.join(dirname, \"index.pkl\")\n",
    "            mutexfile = os.path.join(dirname, \"mutex\")\n",
    "\n",
    "            with FileSystemMutex(mutexfile):\n",
    "                try:\n",
    "                    with open(indexfile, \"rb\") as file:\n",
    "                        index = pickle.load(file)\n",
    "                except FileNotFoundError:\n",
    "                    index = {}\n",
    "\n",
    "                key = (args, frozenset(kwargs), func.__defaults__)\n",
    "\n",
    "                try:\n",
    "                    filename = index[key]\n",
    "                except KeyError:\n",
    "                    index[key] = filename = \"{}.pkl.gz\".format(len(index))\n",
    "                    with open(indexfile, \"wb\") as file:\n",
    "                        pickle.dump(index, file)\n",
    "\n",
    "            filepath = os.path.join(dirname, filename)\n",
    "\n",
    "            try:\n",
    "                with FileSystemMutex(mutexfile):\n",
    "                    with gzip.open(filepath, \"rb\") as file:\n",
    "                        result = pickle.load(file)\n",
    "            except FileNotFoundError:\n",
    "                print(\"compute {}... \".format(filename), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                result = func(*args, **kwargs)\n",
    "                print(\"save {}... \".format(filename), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "                with FileSystemMutex(mutexfile):\n",
    "                    with gzip.open(filepath, \"wb\") as file:\n",
    "                        pickle.dump(result, file)\n",
    "                print(\"done\")\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bec4925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "class torch_default_dtype:\n",
    "\n",
    "    def __init__(self, dtype):\n",
    "        self.saved_dtype = None\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.saved_dtype = torch.get_default_dtype()\n",
    "        torch.set_default_dtype(self.dtype)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        torch.set_default_dtype(self.saved_dtype)\n",
    "        \n",
    "\n",
    "\n",
    "def irr_repr(order, alpha, beta, gamma, dtype=None):\n",
    "    \"\"\"\n",
    "    irreducible representation of SO3\n",
    "    - compatible with compose and spherical_harmonics\n",
    "    \"\"\"\n",
    "    # from from_lielearn_SO3.wigner_d import wigner_D_matrix\n",
    "    from lie_learn.representations.SO3.wigner_d import wigner_D_matrix\n",
    "    # if order == 1:\n",
    "    #     # change of basis to have vector_field[x, y, z] = [vx, vy, vz]\n",
    "    #     A = np.array([[0, 0, 1], [1, 0, 0], [0, 1, 0]])\n",
    "    #     return A @ wigner_D_matrix(1, alpha, beta, gamma) @ A.T\n",
    "\n",
    "    # TODO (non-essential): try to do everything in torch\n",
    "    # return torch.tensor(wigner_D_matrix(torch.tensor(order), alpha, beta, gamma), dtype=torch.get_default_dtype() if dtype is None else dtype)\n",
    "    return torch.tensor(wigner_D_matrix(order, np.array(alpha), np.array(beta), np.array(gamma)), dtype=torch.get_default_dtype() if dtype is None else dtype)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a9e14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4093, -0.1068, -0.9061],\n",
       "        [ 0.1283, -0.9900,  0.0587],\n",
       "        [-0.9033, -0.0922,  0.4189]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### example for irrep\n",
    "# Weigner_D matrix\n",
    "irr_repr(1, 4., 3., 2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7202349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# @profile\n",
    "def kron(a, b):\n",
    "    \"\"\"\n",
    "    A part of the pylabyk library: numpytorch.py at https://github.com/yulkang/pylabyk\n",
    "\n",
    "    Kronecker product of matrices a and b with leading batch dimensions.\n",
    "    Batch dimensions are broadcast. The number of them mush\n",
    "    :type a: torch.Tensor\n",
    "    :type b: torch.Tensor\n",
    "    :rtype: torch.Tensor\n",
    "    \"\"\"\n",
    "    siz1 = torch.Size(torch.tensor(a.shape[-2:]) * torch.tensor(b.shape[-2:]))\n",
    "    res = a.unsqueeze(-1).unsqueeze(-3) * b.unsqueeze(-2).unsqueeze(-4)\n",
    "    siz0 = res.shape[:-4]\n",
    "    return res.reshape(siz0 + siz1)\n",
    "\n",
    "################################################################################\n",
    "# Solving the constraint coming from the stabilizer of 0 and e\n",
    "################################################################################\n",
    "\n",
    "# Get's eigenvectors for eigvalue equel close by eps to zero\n",
    "def get_matrix_kernel(A, eps=1e-10):\n",
    "    '''\n",
    "    Compute an orthonormal basis of the kernel (x_1, x_2, ...)\n",
    "    A x_i = 0\n",
    "    scalar_product(x_i, x_j) = delta_ij\n",
    "\n",
    "    :param A: matrix\n",
    "    :return: matrix where each row is a basis vector of the kernel of A\n",
    "    '''\n",
    "    _u, s, v = torch.svd(A)\n",
    "\n",
    "    # A = u @ torch.diag(s) @ v.t()\n",
    "    kernel = v.t()[s < eps]\n",
    "    return kernel\n",
    "\n",
    "\n",
    "# Stacks the matrix to big matrix and does the same as previous function\n",
    "def get_matrices_kernel(As, eps=1e-10):\n",
    "    '''\n",
    "    Computes the commun kernel of all the As matrices\n",
    "    '''\n",
    "    return get_matrix_kernel(torch.cat(As, dim=0), eps)\n",
    "\n",
    "\n",
    "@cached_dirpklgz(\"cache/trans_Q\")\n",
    "def _basis_transformation_Q_J(J, order_in, order_out, version=3):  # pylint: disable=W0613\n",
    "    \"\"\"\n",
    "    :param J: order of the spherical harmonics\n",
    "    :param order_in: order of the input representation\n",
    "    :param order_out: order of the output representation\n",
    "    :return: one part of the Q^-1 matrix of the article\n",
    "    \"\"\"\n",
    "    with torch_default_dtype(torch.float64):\n",
    "        def _R_tensor(a, b, c): return kron(irr_repr(order_out, a, b, c), irr_repr(order_in, a, b, c))\n",
    "\n",
    "        def _sylvester_submatrix(J, a, b, c):\n",
    "            ''' generate Kronecker product matrix for solving the Sylvester equation in subspace J '''\n",
    "            R_tensor = _R_tensor(a, b, c)  # [m_out * m_in, m_out * m_in]\n",
    "            R_irrep_J = irr_repr(J, a, b, c)  # [m, m]\n",
    "            return kron(R_tensor, torch.eye(R_irrep_J.size(0))) - \\\n",
    "                kron(torch.eye(R_tensor.size(0)), R_irrep_J.t())  # [(m_out * m_in) * m, (m_out * m_in) * m]\n",
    "        \n",
    "        # some random angles to enshure equivariance\n",
    "        random_angles = [\n",
    "            [4.41301023, 5.56684102, 4.59384642],\n",
    "            [4.93325116, 6.12697327, 4.14574096],\n",
    "            [0.53878964, 4.09050444, 5.36539036],\n",
    "            [2.16017393, 3.48835314, 5.55174441],\n",
    "            [2.52385107, 0.2908958, 3.90040975]\n",
    "        ]\n",
    "        null_space = get_matrices_kernel([_sylvester_submatrix(J, a, b, c) for a, b, c in random_angles])\n",
    "        assert null_space.size(0) == 1, null_space.size()  # unique subspace solution\n",
    "        Q_J = null_space[0]  # [(m_out * m_in) * m]\n",
    "        Q_J = Q_J.view((2 * order_out + 1) * (2 * order_in + 1), 2 * J + 1)  # [m_out * m_in, m]\n",
    "        assert all(torch.allclose(_R_tensor(a, b, c) @ Q_J, Q_J @ irr_repr(J, a, b, c)) for a, b, c in torch.rand(4, 3))\n",
    "\n",
    "    assert Q_J.dtype == torch.float64\n",
    "    return Q_J  # [m_out * m_in, m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0954874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " _basis_transformation_Q_J(1, 1, 1, version=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf9e9a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l tensor shape torch.Size([3, 3])\n",
      "k tensor shape torch.Size([5, 5])\n",
      "Kron shape torch.Size([15, 15])\n"
     ]
    }
   ],
   "source": [
    "# some code deconstruction of Q^lk_j^{-1}\n",
    "# helper function that calculates the Kronecker product between\n",
    "# type-k and type-l Wigner-D matrices for rotations a, b, c\n",
    "def _R_tensor(a, b, c):\n",
    "    # kron calculates the kroneker product between two matrices\n",
    "    # irr_repr returns the irrep from (type, alpha, beta, gamma)\n",
    "    # Remember the order A x B = kron(B, A)\n",
    "    return kron(irr_repr(l, a, b, c), irr_repr(k, a, b, c))\n",
    "\n",
    "l = 1\n",
    "k = 2\n",
    "a = 4.; b = 3.; c = 2.;\n",
    "\n",
    "print(\"l tensor shape\", irr_repr(l, a, b, c).shape)\n",
    "print(\"k tensor shape\", irr_repr(k, a, b, c).shape)\n",
    "print(\"Kron shape\", _R_tensor(a, b, c).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09be0911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([105, 105])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computes submatrix to solve sylvester equation\n",
    "# AX - XB = 0\n",
    "# same as (I x A - B^T x I) vec(X) = 0\n",
    "def _sylvester_submatrix(J, a, b, c):\n",
    "    # Calculates the Kroneker product between type-l and type-k\n",
    "    # Wigner-D matrices for rotation angles a, b, c\n",
    "    R_tensor = _R_tensor(a, b, c) # [(2l + 1)*(2k + 1), (2l + 1)*(2k + 1)]\n",
    "    # Calculates type-J Wigner-D matrix for same rotation\n",
    "    R_irrep_J = irr_repr(J, a, b, c) # [2J + 1, 2J + 1]\n",
    "    # .reshape(9).reshape(3, 3) Annoying stuff due to some torch bug with data placement in memory\n",
    "    return kron(R_tensor, torch.eye(R_irrep_J.size(0))) - kron(torch.eye(R_tensor.size(0)), R_irrep_J.t())\n",
    "\n",
    "J = 3\n",
    "\n",
    "_sylvester_submatrix(J, a, b, c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff9c828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check on random angles\n",
    "with torch_default_dtype(torch.float64): # !!! Important otherwise zero\n",
    "    # some random angles to enshure equivariance\n",
    "    random_angles = [\n",
    "        [4.41301023, 5.56684102, 4.59384642],\n",
    "        [4.93325116, 6.12697327, 4.14574096],\n",
    "        [0.53878964, 4.09050444, 5.36539036],\n",
    "        [2.16017393, 3.48835314, 5.55174441],\n",
    "        [2.52385107, 0.2908958, 3.90040975]\n",
    "    ]\n",
    "    # Calculate the vector that is solution of the homogeneous equation\n",
    "    # for all sets of angles\n",
    "    null_space = get_matrices_kernel([_sylvester_submatrix(J, a, b, c)\n",
    "                                      for a, b, c in random_angles])\n",
    "    # confirm that the solution is unique\n",
    "    assert null_space.size(0) == 1, null_space.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "522936ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final Q^lk_J compute and reshape to (2 * l + 1) * (2 * k + 1) * (2 * J + 1)\n",
    "with torch_default_dtype(torch.float64): # !!! Important otherwise zero\n",
    "    Q_J = null_space[0] # only one vector\n",
    "    Q_J = Q_J.view((2 * l + 1) * (2 * k + 1), 2 * J + 1) # unvectorize\n",
    "    assert all(torch.allclose(_R_tensor(a, b, c) @ Q_J, Q_J @ irr_repr(J, a, b, c)) \n",
    "               for a, b, c in torch.rand(4, 3)) # sanity check that is a solution\n",
    "    \n",
    "Q_J.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b728c8ab",
   "metadata": {},
   "source": [
    "### Spherical harmonics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5891367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import lpmv as lpmv_scipy\n",
    "\n",
    "\n",
    "def semifactorial(x):\n",
    "    \"\"\"Compute the semifactorial function x!!.\n",
    "\n",
    "    x!! = x * (x-2) * (x-4) *...\n",
    "\n",
    "    Args:\n",
    "        x: positive int\n",
    "    Returns:\n",
    "        float for x!!\n",
    "    \"\"\"\n",
    "    y = 1.\n",
    "    for n in range(x, 1, -2):\n",
    "        y *= n\n",
    "    return y\n",
    "\n",
    "\n",
    "def pochhammer(x, k):\n",
    "    \"\"\"Compute the pochhammer symbol (x)_k.\n",
    "\n",
    "    (x)_k = x * (x+1) * (x+2) *...* (x+k-1)\n",
    "\n",
    "    Args:\n",
    "        x: positive int\n",
    "    Returns:\n",
    "        float for (x)_k\n",
    "    \"\"\"\n",
    "    xf = float(x)\n",
    "    for n in range(x+1, x+k):\n",
    "        xf *= n\n",
    "    return xf\n",
    "\n",
    "def lpmv(l, m, x):\n",
    "    \"\"\"Associated Legendre function including Condon-Shortley phase.\n",
    "\n",
    "    Args:\n",
    "        m: int order \n",
    "        l: int degree\n",
    "        x: float argument tensor\n",
    "    Returns:\n",
    "        tensor of x-shape\n",
    "    \"\"\"\n",
    "    m_abs = abs(m)\n",
    "    \n",
    "    # P^m_J = 0 forall m > J\n",
    "    if m_abs > l:\n",
    "        return torch.zeros_like(x)\n",
    "\n",
    "    # Compute P_m^m\n",
    "    # P_m^m = (-1)^J (1 - x^2)^(J/2) (2J - 1)!!\n",
    "    yold = ((-1)**m_abs * semifactorial(2*m_abs-1)) * torch.pow(1-x*x, m_abs/2)\n",
    "    \n",
    "    # Compute P_{m+1}^m\n",
    "    # P_m+1^m = x (2 m + 1) P^m_m\n",
    "    if m_abs != l:\n",
    "        y = x * (2*m_abs+1) * yold\n",
    "    else:\n",
    "        y = yold\n",
    "\n",
    "    # Compute P_{l}^m from recursion in P_{l-1}^m and P_{l-2}^m\n",
    "    # P_l^m (x) = [(2 l - 1 ) / ( l - m )] P^m_{l - 1} (x) - [(l + m - 1) / (l - m)] P^m_{l - 2} (x)\n",
    "    for i in range(m_abs+2, l+1):\n",
    "        tmp = y\n",
    "        # Inplace speedup\n",
    "        y = ((2*i-1) / (i-m_abs)) * x * y\n",
    "        y -= ((i+m_abs-1)/(i-m_abs)) * yold\n",
    "        yold = tmp\n",
    "\n",
    "    # P^-m_l (x) = (-1)^m (l - m)!/(l + m)! P^m_l (x)\n",
    "    if m < 0:\n",
    "        y *= ((-1)**m / pochhammer(l+m+1, -2*m))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23fc766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = {}\n",
    "J = 10\n",
    "m = 5\n",
    "x = torch.Tensor([0.5, 0.1, 0.2])\n",
    "ans_1 = lpmv(J, m, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73b4b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalHarmonics(object):\n",
    "    def __init__(self):\n",
    "        self.leg = {}\n",
    "\n",
    "    def clear(self):\n",
    "        self.leg = {}\n",
    "\n",
    "    def negative_lpmv(self, l, m, y):\n",
    "        \"\"\"Compute negative order coefficients\"\"\"\n",
    "        if m < 0:\n",
    "            y *= ((-1)**m / pochhammer(l+m+1, -2*m))\n",
    "        return y\n",
    "\n",
    "    def lpmv(self, l, m, x):\n",
    "        \"\"\"Associated Legendre function including Condon-Shortley phase.\n",
    "\n",
    "        Args:\n",
    "            m: int order \n",
    "            l: int degree\n",
    "            x: float argument tensor\n",
    "        Returns:\n",
    "            tensor of x-shape\n",
    "        \"\"\"\n",
    "        # Check memoized versions\n",
    "        m_abs = abs(m)\n",
    "        if (l,m) in self.leg:\n",
    "            return self.leg[(l,m)]\n",
    "        elif m_abs > l:\n",
    "            return None\n",
    "        elif l == 0:\n",
    "            self.leg[(l,m)] = torch.ones_like(x)\n",
    "            return self.leg[(l,m)]\n",
    "        \n",
    "        # Check if on boundary else recurse solution down to boundary\n",
    "        if m_abs == l:\n",
    "            # Compute P_m^m\n",
    "            y = (-1)**m_abs * semifactorial(2*m_abs-1)\n",
    "            y *= torch.pow(1-x*x, m_abs/2)\n",
    "            self.leg[(l,m)] = self.negative_lpmv(l, m, y)\n",
    "            return self.leg[(l,m)]\n",
    "        else:\n",
    "            # Recursively precompute lower degree harmonics\n",
    "            self.lpmv(l-1, m, x)\n",
    "\n",
    "        # Compute P_{l}^m from recursion in P_{l-1}^m and P_{l-2}^m\n",
    "        # Inplace speedup\n",
    "        y = ((2*l-1) / (l-m_abs)) * x * self.lpmv(l-1, m_abs, x)\n",
    "        if l - m_abs > 1:\n",
    "            y -= ((l+m_abs-1)/(l-m_abs)) * self.leg[(l-2, m_abs)]\n",
    "        #self.leg[(l, m_abs)] = y\n",
    "        \n",
    "        if m < 0:\n",
    "            y = self.negative_lpmv(l, m, y)\n",
    "        self.leg[(l,m)] = y\n",
    "\n",
    "        return self.leg[(l,m)]\n",
    "\n",
    "    def get_element(self, l, m, theta, phi):\n",
    "        \"\"\"Tesseral spherical harmonic with Condon-Shortley phase.\n",
    "\n",
    "        The Tesseral spherical harmonics are also known as the real spherical\n",
    "        harmonics.\n",
    "\n",
    "        Args:\n",
    "            l: int for degree\n",
    "            m: int for order, where -l <= m < l\n",
    "            theta: collatitude or polar angle\n",
    "            phi: longitude or azimuth\n",
    "        Returns:\n",
    "            tensor of shape theta\n",
    "        \"\"\"\n",
    "        assert abs(m) <= l, \"absolute value of order m must be <= degree l\"\n",
    "\n",
    "        N = np.sqrt((2*l+1) / (4*np.pi))\n",
    "        leg = self.lpmv(l, abs(m), torch.cos(theta))\n",
    "        if m == 0:\n",
    "            return N*leg\n",
    "        elif m > 0:\n",
    "            Y = torch.cos(m*phi) * leg\n",
    "        else:\n",
    "            Y = torch.sin(abs(m)*phi) * leg\n",
    "        N *= np.sqrt(2. / pochhammer(l-abs(m)+1, 2*abs(m)))\n",
    "        Y *= N\n",
    "        return Y\n",
    "\n",
    "    def get(self, l, theta, phi, refresh=True):\n",
    "        \"\"\"Tesseral harmonic with Condon-Shortley phase.\n",
    "\n",
    "        The Tesseral spherical harmonics are also known as the real spherical\n",
    "        harmonics.\n",
    "\n",
    "        Args:\n",
    "            l: int for degree\n",
    "            theta: collatitude or polar angle\n",
    "            phi: longitude or azimuth\n",
    "        Returns:\n",
    "            tensor of shape [*theta.shape, 2*l+1]\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        if refresh:\n",
    "            self.clear()\n",
    "        for m in range(-l, l+1):\n",
    "            results.append(self.get_element(l, m, theta, phi))\n",
    "        return torch.stack(results, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a80d7bc",
   "metadata": {},
   "source": [
    "### My own implementation from spherical harmonics from the blog post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73018331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will implement ALP (Associated Legandre Polynomials)\n",
    "\n",
    "# function analagous to pochammer function in the SE(3) Transformer\n",
    "# (J - m)!/(J + m)!\n",
    "def falling_factorial(J, m):\n",
    "    # computes (J + m)*(J+m - 1)*...(J-m+1)\n",
    "    f = 1.\n",
    "    for n in range(J + m, J - m, -1):\n",
    "        f *= n\n",
    "    return f\n",
    "\n",
    "\n",
    "def semifactorial(x):\n",
    "    \"\"\"Compute the semifactorial function x!!.\n",
    "\n",
    "    x!! = x * (x-2) * (x-4) *...\n",
    "\n",
    "    Args:\n",
    "        x: positive int\n",
    "    Returns:\n",
    "        float for x!!\n",
    "    \"\"\"\n",
    "    y = 1.\n",
    "    for n in range(x, 1, -2):\n",
    "        y *= n\n",
    "    return y\n",
    "\n",
    "# y: Legendre polynimil for the absolute value of m\n",
    "# P_J^{-m} (x) = (-1)^m (J - m)!/(J + m)! P_J^m(x)\n",
    "def negative_lpmv(J, m, y):\n",
    "    # check if m is negative\n",
    "    if m < 0:\n",
    "        # multiply y with the coefficient containing the falling \n",
    "        # factorial\n",
    "        y *= ((-1)**m / falling_factorial(J, m))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b35f218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628800.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "falling_factorial(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24c6b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive implementation of APL\n",
    "def lpmv(J, m, x):\n",
    "    # get the absolute value of m\n",
    "    m_abs = abs(m)\n",
    "    # check if the polynomial has already been computed\n",
    "    if (J, m) in leg:\n",
    "        return leg[(J, m)]\n",
    "    # check if m is out of range -J to J\n",
    "    elif m_abs > J:\n",
    "        return None\n",
    "    # if J = 0, the associated Legendre polynomial is equal to 1\n",
    "    elif J == 0:\n",
    "        # return tensor of 1s with the same shape as x\n",
    "        leg[(l, m)] = torch.ones_like(x)\n",
    "        return leg[(l, m)]\n",
    "    \n",
    "    # if |m| = J, compute the polynomial using the equation from step 1\n",
    "    if m_abs == J:\n",
    "        # P^J_J (x) = (-1)^J (1 - x^2)^(J/2) (2J - 1)!!\n",
    "        # calculate coefficient term\n",
    "        y = (-1)**J * semifactorial(2*J - 1)\n",
    "        # multiply by the term dependent on x\n",
    "        y *= torch.pow(1 - x*x, m_abs/2)\n",
    "        # negative_lpmv returns y if m is positive and y multiplied by \n",
    "        # the negative coefficient defined in step 4 if m is negative\n",
    "        leg[(l, m)] = negative_lpmv(l, m, y)\n",
    "        return leg[(l, m)]\n",
    "    else:\n",
    "        # retursive call to compute lower degree polynomials up to\n",
    "        # boundary m = J\n",
    "        lpmv(J - 1, m, x)\n",
    "        \n",
    "    # if m is not on the boundary, first compute the first term of the relation\n",
    "    # defined in step 3\n",
    "    # P^J-1_J (x) = x (2J + 1) P^m_(J - 1) (x)\n",
    "    # if m_abs = J - 1, then this calculates the relation defined in step 2\n",
    "    y = ((2*J - 1)/(J - m_abs)) * x * lpmv(J - 1, m_abs, x)\n",
    "    \n",
    "    # P_J^m(x) = (2J - 1)/(J - m) x P^m_{J - 1}(x) - (J + m - 1)/(J - m) P^m_{J - 2} (x)\n",
    "    # check if m_abs != J - 1, then add the second term defined in step 3\n",
    "    if l - m_abs > 1:\n",
    "        y -= ((l + m_abs -1)/(l - m_abs)) * leg[(l - 2, m_abs)]\n",
    "        \n",
    "    # if m is negative, return the polynomial for m_abs scaled by the \n",
    "    # negative coefficient\n",
    "    if m < 0:\n",
    "        y = negative_lpmv(l, m, y)\n",
    "        \n",
    "    leg[(l, m)] = y\n",
    "    \n",
    "    return leg[(l, m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ff945e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = {}\n",
    "J = 10\n",
    "m = 5\n",
    "x = torch.Tensor([0.5, 0.1, 0.2])\n",
    "ans_2 = lpmv(J, m, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2257006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 30086.1719, -21961.9492, -26591.5078])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a51da14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.3058e+04, -5.3207e+01, -1.5766e+03])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ba7b57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spherical harmonics\n",
    "\n",
    "def get_element(J, m, theta, phi):\n",
    "    assert abs(m) <= J, \"m must be in the range -J to J\"\n",
    "    \n",
    "    # calculates the first fraction in the square root\n",
    "    N = np.sqrt((2*J + 1) / (4 * np.pi))\n",
    "    # stores the ALP term in leg\n",
    "    leg = lpmv(J, abs(m), torch.cos(theta))\n",
    "    \n",
    "    # multiply by the phi dependent term depending on the value of m\n",
    "    if m == 0:\n",
    "        # when m = 0 the other fraction in the square root cancels \n",
    "        # and the phi dependent term is 1\n",
    "        return N * leg\n",
    "    elif m > 0:\n",
    "        Y = torch.cos(m*phi) * leg\n",
    "    else:\n",
    "        #print(phi.shape, leg.shape)\n",
    "        Y = torch.sin(abs(m) * phi) * leg\n",
    "        \n",
    "    # multiply by a square root of the inverse falling factorial\n",
    "    # which is the same as in the ALP\n",
    "    N *= np.sqrt(2. / falling_factorial(J, abs(m)))\n",
    "    # multiplies the coefficient with angle-dependent term\n",
    "    Y *= N\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d148a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Y_J^m, m in {-J, J}\n",
    "def get(J, theta, phi, refresh = True):\n",
    "    # initialize tensor\n",
    "    results = []\n",
    "    \n",
    "    # loop over all possible values of m from J to J and add the \n",
    "    # computed spherical harmonic to results\n",
    "    for m in range(-J, J + 1):\n",
    "        results.append(get_element(J, m, theta, phi))\n",
    "        \n",
    "    return torch.stack(results, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc31874f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.4857e-02,  2.4532e-01, -1.0650e+00,  1.6601e+00, -2.9646e+01,\n",
       "          1.5752e-01, -4.5993e-03],\n",
       "        [-2.6125e-03,  2.1772e-02, -2.5473e-01,  2.5822e+00, -2.6617e+01,\n",
       "          5.1495e-02, -3.8186e-03],\n",
       "        [-1.7350e-04,  2.8475e-03, -6.7310e-02,  2.7433e+00, -1.4869e+01,\n",
       "          1.4047e-02, -5.6087e-04]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leg = {}\n",
    "\n",
    "get(3, torch.Tensor([0.5, 0.2, 0.1]), torch.Tensor([0.5, 0.2, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c3854bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r_ij: the relative displacement between nodes in spherical\n",
    "# coordinates [radius, alpha, beta]\n",
    "# beta = pi - theta (beta is 0 at south pole and pi at north pole;\n",
    "# supplementary to theta\n",
    "# alpha = phi (ranges from 0 to 2 pi)\n",
    "# r_ij: shape (batch_size, nodes, neighbors, 3 (r_ij))\n",
    "def precompute_sh(r_ij, max_J):\n",
    "    # initialize dictionary where keys correspond to J and values\n",
    "    # are tensors with shape (batch size, nodes, neighbors, 2J + 1)\n",
    "    Y_Js = {}\n",
    "    \n",
    "    # calculate (2J + 1)-dimensional spherical harmonics tensors for degrees up to\n",
    "    # max_J\n",
    "    for J in range(max_J + 1):\n",
    "        # r_ij[..., 2] extracts the values for beta for every edge in\n",
    "        # the graph\n",
    "        # r_ij[..., 1] extracts the values for alpha for every edge in the graph\n",
    "        Y_Js[J] = get(J, theta = math.pi - r_ij[..., 2], phi = r_ij[..., 1], refresh = False)\n",
    "        \n",
    "        \n",
    "    return Y_Js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df734f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3]) torch.Size([10, 1, 10, 5])\n"
     ]
    }
   ],
   "source": [
    "leg = {}\n",
    "\n",
    "r_ij = torch.randn((10, 1, 10, 3))/100\n",
    "\n",
    "out = precompute_sh(r_ij, 3)\n",
    "\n",
    "print(out.keys(), out[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea857ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basis_kernel(x_ij, max_degree = 2):\n",
    "\n",
    "    # compute all spherical harmonics for every edge up to 2*maximum \n",
    "    # feature type\n",
    "\n",
    "    Y = precompute_sh(x_ij, 2*max_degree)\n",
    "    device = Y[0].device\n",
    "\n",
    "    # initialize the dictionary where the key is the input and output degree \n",
    "    # pair and the values are all the basis kernels stored in an array of shape\n",
    "    # (edges, 1, 2l+1, 1, 2k+1, 2min(l, k) + 1)\n",
    "    basis = {}\n",
    "    # loop through input and output degree pairs up to max_degree\n",
    "    for di in range(max_degree + 1):\n",
    "        for do in range(max_degree + 1):\n",
    "            K_Js = [] # initialize set of basis kernels\n",
    "            # loop through all values of J from |k - l| to k + l\n",
    "            for J in range(abs(di - do), di + do + 1):\n",
    "                # get change-of-basis matrices with shape \n",
    "                # ((2l + 1)*(2k + 1), 2J + 1) that transforms the (2J + 1)-dim spherical\n",
    "                # tensor back to its original basis\n",
    "                Q_J = _basis_transformation_Q_J(J, di, do)\n",
    "                \n",
    "                Q_J = Q_J.float().to(device)\n",
    "                # Y[J] has shape (edges, 2J + 1)\n",
    "                # Q_J has shape ((2l + 1)*(2k+1), 2J + 1)\n",
    "                # matrix-vector multiplication to get K_J with shape\n",
    "                # (edges, (2l + 1)*(2k + 1)) of the vectorized type-J basis kernels\n",
    "                # W_lk = Q^lk_J @ Y_J = \\sum_J Q^lk_J Y_J\n",
    "                #print(Q_J.shape, Y[J].shape)\n",
    "                K_J = torch.matmul(Q_J, Y[J].t())\n",
    "                # Append to list of bases with shapes (2min(l, k) + 1, edges, (2l+1)*(2k+1))\n",
    "                K_Js.append(K_J)\n",
    "                \n",
    "            # reshape for dot product with radial weights\n",
    "            size = (-1, 1, 2*do + 1, 1, 2*di + 1, 2*min(di, do) + 1)\n",
    "            # stack reshapes to (edges, (2l+1)*(2k + 1), 2min(l, k) + 1)\n",
    "            # view reshapes to match size\n",
    "            basis[f'{di}, {do}'] = torch.stack(K_Js, -1).view(*size)\n",
    "            \n",
    "    return basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5da01d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 3, 1, 3, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leg = {}\n",
    "x_ij =  torch.randn((100, 3))/100\n",
    "\n",
    "get_basis_kernel(x_ij)['1, 1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d34230",
   "metadata": {},
   "source": [
    "### Cartezian to spherical convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fd901c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spherical_from_cartesian_torch(cartesian, divide_radius_by=1.0):\n",
    "\n",
    "    ###################################################################################################################\n",
    "    # ON ANGLE CONVENTION\n",
    "    #\n",
    "    # sh has following convention for angles:\n",
    "    # :param theta: the colatitude / polar angle, ranging from 0(North Pole, (X, Y, Z) = (0, 0, 1)) to pi(South Pole, (X, Y, Z) = (0, 0, -1)).\n",
    "    # :param phi: the longitude / azimuthal angle, ranging from 0 to 2 pi.\n",
    "    #\n",
    "    # the 3D steerable CNN code therefore (probably) has the following convention for alpha and beta:\n",
    "    # beta = pi - theta; ranging from 0(South Pole, (X, Y, Z) = (0, 0, -1)) to pi(North Pole, (X, Y, Z) = (0, 0, 1)).\n",
    "    # alpha = phi\n",
    "    #\n",
    "    ###################################################################################################################\n",
    "\n",
    "    # initialise return array\n",
    "    # ptsnew = np.hstack((xyz, np.zeros(xyz.shape)))\n",
    "    spherical = torch.zeros_like(cartesian)\n",
    "\n",
    "    # indices for return array\n",
    "    ind_radius = 0\n",
    "    ind_alpha = 1\n",
    "    ind_beta = 2\n",
    "\n",
    "    cartesian_x = 2\n",
    "    cartesian_y = 0\n",
    "    cartesian_z = 1\n",
    "\n",
    "    # get projected radius in xy plane\n",
    "    # xy = xyz[:,0]**2 + xyz[:,1]**2\n",
    "    r_xy = cartesian[..., cartesian_x] ** 2 + cartesian[..., cartesian_y] ** 2\n",
    "\n",
    "    # get second angle\n",
    "    # version 'elevation angle defined from Z-axis down'\n",
    "    spherical[..., ind_beta] = torch.atan2(torch.sqrt(r_xy), cartesian[..., cartesian_z])\n",
    "    # ptsnew[:,4] = np.arctan2(np.sqrt(xy), xyz[:,2])\n",
    "    # version 'elevation angle defined from XY-plane up'\n",
    "    #ptsnew[:,4] = np.arctan2(xyz[:,2], np.sqrt(xy))\n",
    "    # spherical[:, ind_beta] = np.arctan2(cartesian[:, 2], np.sqrt(r_xy))\n",
    "\n",
    "    # get angle in x-y plane\n",
    "    spherical[...,ind_alpha] = torch.atan2(cartesian[...,cartesian_y], cartesian[...,cartesian_x])\n",
    "\n",
    "    # get overall radius\n",
    "    # ptsnew[:,3] = np.sqrt(xy + xyz[:,2]**2)\n",
    "    if divide_radius_by == 1.0:\n",
    "        spherical[..., ind_radius] = torch.sqrt(r_xy + cartesian[...,cartesian_z]**2)\n",
    "    else:\n",
    "        spherical[..., ind_radius] = torch.sqrt(r_xy + cartesian[...,cartesian_z]**2)/divide_radius_by\n",
    "\n",
    "    return spherical\n",
    "\n",
    "\n",
    "# @profile\n",
    "def get_spherical_from_cartesian(cartesian):\n",
    "\n",
    "    ###################################################################################################################\n",
    "    # ON ANGLE CONVENTION\n",
    "    #\n",
    "    # sh has following convention for angles:\n",
    "    # :param theta: the colatitude / polar angle, ranging from 0(North Pole, (X, Y, Z) = (0, 0, 1)) to pi(South Pole, (X, Y, Z) = (0, 0, -1)).\n",
    "    # :param phi: the longitude / azimuthal angle, ranging from 0 to 2 pi.\n",
    "    #\n",
    "    # the 3D steerable CNN code therefore (probably) has the following convention for alpha and beta:\n",
    "    # beta = pi - theta; ranging from 0(South Pole, (X, Y, Z) = (0, 0, -1)) to pi(North Pole, (X, Y, Z) = (0, 0, 1)).\n",
    "    # alpha = phi\n",
    "    #\n",
    "    ###################################################################################################################\n",
    "\n",
    "    if torch.is_tensor(cartesian):\n",
    "        cartesian = np.array(cartesian.cpu())\n",
    "\n",
    "    # initialise return array\n",
    "    # ptsnew = np.hstack((xyz, np.zeros(xyz.shape)))\n",
    "    spherical = np.zeros(cartesian.shape)\n",
    "\n",
    "    # indices for return array\n",
    "    ind_radius = 0\n",
    "    ind_alpha = 1\n",
    "    ind_beta = 2\n",
    "\n",
    "    cartesian_x = 2\n",
    "    cartesian_y = 0\n",
    "    cartesian_z = 1\n",
    "\n",
    "    # get projected radius in xy plane\n",
    "    # xy = xyz[:,0]**2 + xyz[:,1]**2\n",
    "    r_xy = cartesian[..., cartesian_x] ** 2 + cartesian[..., cartesian_y] ** 2\n",
    "\n",
    "    # get overall radius\n",
    "    # ptsnew[:,3] = np.sqrt(xy + xyz[:,2]**2)\n",
    "    spherical[..., ind_radius] = np.sqrt(r_xy + cartesian[...,cartesian_z]**2)\n",
    "\n",
    "    # get second angle\n",
    "    # version 'elevation angle defined from Z-axis down'\n",
    "    spherical[..., ind_beta] = np.arctan2(np.sqrt(r_xy), cartesian[..., cartesian_z])\n",
    "    # ptsnew[:,4] = np.arctan2(np.sqrt(xy), xyz[:,2])\n",
    "    # version 'elevation angle defined from XY-plane up'\n",
    "    #ptsnew[:,4] = np.arctan2(xyz[:,2], np.sqrt(xy))\n",
    "    # spherical[:, ind_beta] = np.arctan2(cartesian[:, 2], np.sqrt(r_xy))\n",
    "\n",
    "    # get angle in x-y plane\n",
    "    spherical[...,ind_alpha] = np.arctan2(cartesian[...,cartesian_y], cartesian[...,cartesian_x])\n",
    "\n",
    "    return spherical\n",
    "\n",
    "def test_coordinate_conversion():\n",
    "    p = np.array([0, 0, -1])\n",
    "    expected = np.array([1, 0, 0])\n",
    "    assert get_spherical_from_cartesian(p) == expected\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76425a47",
   "metadata": {},
   "source": [
    "### Reference implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39c38302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import lpmv as lpmv_scipy\n",
    "\n",
    "\n",
    "def semifactorial(x):\n",
    "    \"\"\"Compute the semifactorial function x!!.\n",
    "\n",
    "    x!! = x * (x-2) * (x-4) *...\n",
    "\n",
    "    Args:\n",
    "        x: positive int\n",
    "    Returns:\n",
    "        float for x!!\n",
    "    \"\"\"\n",
    "    y = 1.\n",
    "    for n in range(x, 1, -2):\n",
    "        y *= n\n",
    "    return y\n",
    "\n",
    "\n",
    "def pochhammer(x, k):\n",
    "    \"\"\"Compute the pochhammer symbol (x)_k.\n",
    "\n",
    "    (x)_k = x * (x+1) * (x+2) *...* (x+k-1)\n",
    "\n",
    "    Args:\n",
    "        x: positive int\n",
    "    Returns:\n",
    "        float for (x)_k\n",
    "    \"\"\"\n",
    "    xf = float(x)\n",
    "    for n in range(x+1, x+k):\n",
    "        xf *= n\n",
    "    return xf\n",
    "\n",
    "def lpmv(l, m, x):\n",
    "    \"\"\"Associated Legendre function including Condon-Shortley phase.\n",
    "\n",
    "    Args:\n",
    "        m: int order \n",
    "        l: int degree\n",
    "        x: float argument tensor\n",
    "    Returns:\n",
    "        tensor of x-shape\n",
    "    \"\"\"\n",
    "    m_abs = abs(m)\n",
    "    \n",
    "    # P^m_J = 0 forall m > J\n",
    "    if m_abs > l:\n",
    "        return torch.zeros_like(x)\n",
    "\n",
    "    # Compute P_m^m\n",
    "    # P_m^m = (-1)^J (1 - x^2)^(J/2) (2J - 1)!!\n",
    "    yold = ((-1)**m_abs * semifactorial(2*m_abs-1)) * torch.pow(1-x*x, m_abs/2)\n",
    "    \n",
    "    # Compute P_{m+1}^m\n",
    "    # P_m+1^m = x (2 m + 1) P^m_m\n",
    "    if m_abs != l:\n",
    "        y = x * (2*m_abs+1) * yold\n",
    "    else:\n",
    "        y = yold\n",
    "\n",
    "    # Compute P_{l}^m from recursion in P_{l-1}^m and P_{l-2}^m\n",
    "    # P_l^m (x) = [(2 l - 1 ) / ( l - m )] P^m_{l - 1} (x) - [(l + m - 1) / (l - m)] P^m_{l - 2} (x)\n",
    "    for i in range(m_abs+2, l+1):\n",
    "        tmp = y\n",
    "        # Inplace speedup\n",
    "        y = ((2*i-1) / (i-m_abs)) * x * y\n",
    "        y -= ((i+m_abs-1)/(i-m_abs)) * yold\n",
    "        yold = tmp\n",
    "\n",
    "    # P^-m_l (x) = (-1)^m (l - m)!/(l + m)! P^m_l (x)\n",
    "    if m < 0:\n",
    "        y *= ((-1)**m / pochhammer(l+m+1, -2*m))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd20b7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SphericalHarmonics(object):\n",
    "    def __init__(self):\n",
    "        self.leg = {}\n",
    "\n",
    "    def clear(self):\n",
    "        self.leg = {}\n",
    "\n",
    "    def negative_lpmv(self, l, m, y):\n",
    "        \"\"\"Compute negative order coefficients\"\"\"\n",
    "        if m < 0:\n",
    "            y *= ((-1)**m / pochhammer(l+m+1, -2*m))\n",
    "        return y\n",
    "\n",
    "    def lpmv(self, l, m, x):\n",
    "        \"\"\"Associated Legendre function including Condon-Shortley phase.\n",
    "\n",
    "        Args:\n",
    "            m: int order \n",
    "            l: int degree\n",
    "            x: float argument tensor\n",
    "        Returns:\n",
    "            tensor of x-shape\n",
    "        \"\"\"\n",
    "        # Check memoized versions\n",
    "        m_abs = abs(m)\n",
    "        if (l,m) in self.leg:\n",
    "            return self.leg[(l,m)]\n",
    "        elif m_abs > l:\n",
    "            return None\n",
    "        elif l == 0:\n",
    "            self.leg[(l,m)] = torch.ones_like(x)\n",
    "            return self.leg[(l,m)]\n",
    "        \n",
    "        # Check if on boundary else recurse solution down to boundary\n",
    "        if m_abs == l:\n",
    "            # Compute P_m^m\n",
    "            y = (-1)**m_abs * semifactorial(2*m_abs-1)\n",
    "            y *= torch.pow(1-x*x, m_abs/2)\n",
    "            self.leg[(l,m)] = self.negative_lpmv(l, m, y)\n",
    "            return self.leg[(l,m)]\n",
    "        else:\n",
    "            # Recursively precompute lower degree harmonics\n",
    "            self.lpmv(l-1, m, x)\n",
    "\n",
    "        # Compute P_{l}^m from recursion in P_{l-1}^m and P_{l-2}^m\n",
    "        # Inplace speedup\n",
    "        y = ((2*l-1) / (l-m_abs)) * x * self.lpmv(l-1, m_abs, x)\n",
    "        if l - m_abs > 1:\n",
    "            y -= ((l+m_abs-1)/(l-m_abs)) * self.leg[(l-2, m_abs)]\n",
    "        #self.leg[(l, m_abs)] = y\n",
    "        \n",
    "        if m < 0:\n",
    "            y = self.negative_lpmv(l, m, y)\n",
    "        self.leg[(l,m)] = y\n",
    "\n",
    "        return self.leg[(l,m)]\n",
    "\n",
    "    def get_element(self, l, m, theta, phi):\n",
    "        \"\"\"Tesseral spherical harmonic with Condon-Shortley phase.\n",
    "\n",
    "        The Tesseral spherical harmonics are also known as the real spherical\n",
    "        harmonics.\n",
    "\n",
    "        Args:\n",
    "            l: int for degree\n",
    "            m: int for order, where -l <= m < l\n",
    "            theta: collatitude or polar angle\n",
    "            phi: longitude or azimuth\n",
    "        Returns:\n",
    "            tensor of shape theta\n",
    "        \"\"\"\n",
    "        assert abs(m) <= l, \"absolute value of order m must be <= degree l\"\n",
    "\n",
    "        N = np.sqrt((2*l+1) / (4*np.pi))\n",
    "        leg = self.lpmv(l, abs(m), torch.cos(theta))\n",
    "        if m == 0:\n",
    "            return N*leg\n",
    "        elif m > 0:\n",
    "            Y = torch.cos(m*phi) * leg\n",
    "        else:\n",
    "            Y = torch.sin(abs(m)*phi) * leg\n",
    "        N *= np.sqrt(2. / pochhammer(l-abs(m)+1, 2*abs(m)))\n",
    "        Y *= N\n",
    "        return Y\n",
    "\n",
    "    def get(self, l, theta, phi, refresh=True):\n",
    "        \"\"\"Tesseral harmonic with Condon-Shortley phase.\n",
    "\n",
    "        The Tesseral spherical harmonics are also known as the real spherical\n",
    "        harmonics.\n",
    "\n",
    "        Args:\n",
    "            l: int for degree\n",
    "            theta: collatitude or polar angle\n",
    "            phi: longitude or azimuth\n",
    "        Returns:\n",
    "            tensor of shape [*theta.shape, 2*l+1]\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        if refresh:\n",
    "            self.clear()\n",
    "        for m in range(-l, l+1):\n",
    "            results.append(self.get_element(l, m, theta, phi))\n",
    "        return torch.stack(results, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480bbd28",
   "metadata": {},
   "source": [
    "### Radial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54ad7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BN(nn.Module):\n",
    "    \"\"\"SE(3)-equvariant batch/layer normalization\"\"\"\n",
    "    def __init__(self, m):\n",
    "        \"\"\"SE(3)-equvariant batch/layer normalization\n",
    "\n",
    "        Args:\n",
    "            m: int for number of output channels\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bn = nn.LayerNorm(m)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(x)\n",
    "\n",
    "# num_freq = 2 min (l, k) + 1\n",
    "# mid_dim = 32\n",
    "class RadialFunc(nn.Module):\n",
    "    \"\"\"NN parameterized radial profile function\"\"\"\n",
    "    def __init__(self, num_bases, mi, mo, edge_dim: int = 0):\n",
    "        \"\"\" NN parametrized radial profile function.\n",
    "        \n",
    "        Args:\n",
    "            num_freq: number of output frequencies\n",
    "            in_dim: multiplicity of input (num input channels)\n",
    "            out_dim: multiplicity of output (num output channels)\n",
    "            edge_dim: number of dimensions for edge embedding\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_bases = num_bases\n",
    "        self.mi = mi\n",
    "        self.mid_dim = 32\n",
    "        self.mo = mo\n",
    "        self.edge_dim = edge_dim\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            # FFN transfroms from number of edges to mid_dim\n",
    "            nn.Linear(self.edge_dim + 1, self.mid_dim),\n",
    "            # Normalization of the layer output to zeros mean and std equal to 1\n",
    "            BN(self.mid_dim),\n",
    "            nn.ReLU(),\n",
    "            # Hidden layer that does not change dim\n",
    "            nn.Linear(self.mid_dim, self.mid_dim),\n",
    "            # Another Norm\n",
    "            BN(self.mid_dim),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(),\n",
    "            # FFN transforms from dim_dim to (2min(l, k) + 1)*mi*mo\n",
    "            nn.Linear(self.mid_dim, self.num_bases*mi*mo)\n",
    "        )\n",
    "        \n",
    "        nn.init.kaiming_uniform_(self.net[0].weight)\n",
    "        nn.init.kaiming_uniform_(self.net[3].weight)\n",
    "        nn.init.kaiming_uniform_(self.net[6].weight)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"RadialFunc(edge_dim={self.edge_dim}, in_dim={self.mi}, out_dim={self.mo})\"\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # calculates a single vector of radial weights given the distance\n",
    "        # between nodes with the FFN\n",
    "        y = self.net(x)\n",
    "        # reshapes to separate the radial weights by output channels,\n",
    "        # input channel, and degree J to prepare for broadcasting and element-wise\n",
    "        # multiplication with the array of basis kernels of shape (-1, 1, 2l+1, 1, 2k+1, num_bases)\n",
    "        #print(y.shape)\n",
    "        return y.view(-1, self.mo, 1, self.mi, 1, self.num_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b2cde13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 15, 1, 10, 1, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single test\n",
    "\n",
    "dist_ij = torch.rand((10, 1))\n",
    "\n",
    "model_Radial = RadialFunc(3, 10, 15)\n",
    "\n",
    "model_Radial(dist_ij).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63d729",
   "metadata": {},
   "source": [
    "### Tensor Field Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8aa12413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the radial network for type-k inputs and type-l outputs\n",
    "# the falue of edge_dim has a default value of 1 which determines\n",
    "# the dimension of the input to the radial function\n",
    "num_bases = 3\n",
    "mi = 1\n",
    "mo = 3\n",
    "edge_dim = 0\n",
    "rp = RadialFunc(num_bases, mi, mo, edge_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f540b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calls forward method of RadialFunc class which feeds the realtive\n",
    "# distance into radial network\n",
    "R = rp(dist_ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad4470ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ij =  torch.randn((100, 3))/100\n",
    "\n",
    "r_ij = x_ij[..., 0, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5ceef7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "leg = {}\n",
    "x_ij =  torch.randn((100, 3))/100\n",
    "\n",
    "r_ij = x_ij[..., 0, None]\n",
    "\n",
    "do = 0\n",
    "di = 1\n",
    "\n",
    "basis = get_basis_kernel(x_ij)\n",
    "\n",
    "# 2 min(l, k) + 1\n",
    "num_bases = 2*min(do, di) + 1\n",
    "mi = 1\n",
    "mo = 3\n",
    "edge_dim = 0\n",
    "rp = RadialFunc(num_bases, mi, mo, edge_dim)\n",
    "R = rp(r_ij)\n",
    "\n",
    "\n",
    "# R: radial weights with shape (batch_size, mo, 1, mi, 1, 2 min(di, do) + 1)\n",
    "# basis[f'{self.di}, {self.do}']: tensor of basis kernels \n",
    "# basis_shape:                 (batch_size, 1, 2*do + 1, 1, 2*di + 1, 2*min(di, do) + 1)\n",
    "# for input deg di and output deg do\n",
    "# kernel output:               (batch_size, mo, 2*do + 1, mi, 2*di + 1)\n",
    "kernel = torch.sum(R*basis[f'{di}, {do}'], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "46e12f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape kernel to (mo * (2*do + 1), mi * (2*di + 1)) to prepare for\n",
    "# matrix-vector multiplication with hte concatenated input channels of type\n",
    "# di\n",
    "kernel = kernel.view(kernel.shape[0], (2 * do + 1) * mo, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bed4790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseConv(nn.Module):\n",
    "    \"\"\"SE(3)-equivariant convolution between two single-type features\"\"\"\n",
    "    def __init__(self, degree_in: int, nc_in: int, degree_out: int,\n",
    "                 nc_out: int, edge_dim: int=0):\n",
    "        \"\"\"SE(3)-equivariant convolution between a pair of feature types.\n",
    "\n",
    "        This layer performs a convolution from nc_in features of type degree_in\n",
    "        to nc_out features of type degree_out.\n",
    "\n",
    "        Args:\n",
    "            degree_in: degree of input fiber\n",
    "            nc_in: number of channels on input\n",
    "            degree_out: degree of out order\n",
    "            nc_out: number of channels on output\n",
    "            edge_dim: number of dimensions for edge embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Log settings\n",
    "        self.degree_in = degree_in\n",
    "        self.degree_out = degree_out\n",
    "        self.nc_in = nc_in\n",
    "        self.nc_out = nc_out\n",
    "\n",
    "        # Functions of the degree\n",
    "        self.num_freq = 2*min(degree_in, degree_out) + 1\n",
    "        self.d_out = 2*degree_out + 1\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        # Radial profile function\n",
    "        self.rp = RadialFunc(self.num_freq, nc_in, nc_out, self.edge_dim)\n",
    "        \n",
    "    @profile\n",
    "    def forward(self, feat, basis):\n",
    "        # Get radial weights\n",
    "        R = self.rp(feat)\n",
    "        kernel = torch.sum(R * basis[f'{self.degree_in},{self.degree_out}'], -1)\n",
    "        return kernel.view(kernel.shape[0], self.d_out*self.nc_out, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753f7e6",
   "metadata": {},
   "source": [
    "### Fixing representation to apply fibers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e4fea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@profile\n",
    "def get_basis(G, max_degree, compute_gradients):\n",
    "    \"\"\"Precompute the SE(3)-equivariant weight basis, W_J^lk(x)\n",
    "\n",
    "    This is called by get_basis_and_r().\n",
    "\n",
    "    Args:\n",
    "        G: DGL graph instance of type dgl.DGLGraph\n",
    "        max_degree: non-negative int for degree of highest feature type\n",
    "        compute_gradients: boolean, whether to compute gradients during basis construction\n",
    "    Returns:\n",
    "        dict of equivariant bases. Keys are in the form 'd_in,d_out'. Values are\n",
    "        tensors of shape (batch_size, 1, 2*d_out+1, 1, 2*d_in+1, number_of_bases)\n",
    "        where the 1's will later be broadcast to the number of output and input\n",
    "        channels\n",
    "    \"\"\"\n",
    "    if compute_gradients:\n",
    "        context = nullcontext()\n",
    "    else:\n",
    "        context = torch.no_grad()\n",
    "\n",
    "    with context:\n",
    "        cloned_d = torch.clone(G.edata['d'])\n",
    "\n",
    "        if G.edata['d'].requires_grad:\n",
    "            cloned_d.requires_grad_()\n",
    "            log_gradient_norm(cloned_d, 'Basis computation flow')\n",
    "\n",
    "        # Relative positional encodings (vector)\n",
    "        r_ij = get_spherical_from_cartesian_torch(cloned_d)\n",
    "        # Spherical harmonic basis\n",
    "        Y = precompute_sh(r_ij, 2*max_degree)\n",
    "        device = Y[0].device\n",
    "\n",
    "        basis = {}\n",
    "        for d_in in range(max_degree+1):\n",
    "            for d_out in range(max_degree+1):\n",
    "                K_Js = []\n",
    "                for J in range(abs(d_in-d_out), d_in+d_out+1):\n",
    "                    # Get spherical harmonic projection matrices\n",
    "                    Q_J = _basis_transformation_Q_J(J, d_in, d_out)\n",
    "                    Q_J = Q_J.float().T.to(device)\n",
    "\n",
    "                    # Create kernel from spherical harmonics\n",
    "                    K_J = torch.matmul(Y[J], Q_J)\n",
    "                    K_Js.append(K_J)\n",
    "\n",
    "                # Reshape so can take linear combinations with a dot product\n",
    "                size = (-1, 1, 2*d_out+1, 1, 2*d_in+1, 2*min(d_in, d_out)+1)\n",
    "                basis[f'{d_in},{d_out}'] = torch.stack(K_Js, -1).view(*size)\n",
    "        return basis\n",
    "\n",
    "\n",
    "def get_r(G):\n",
    "    \"\"\"Compute internodal distances\"\"\"\n",
    "    cloned_d = torch.clone(G.edata['d'])\n",
    "\n",
    "    if G.edata['d'].requires_grad:\n",
    "        cloned_d.requires_grad_()\n",
    "        log_gradient_norm(cloned_d, 'Neural networks flow')\n",
    "\n",
    "    return torch.sqrt(torch.sum(cloned_d**2, -1, keepdim=True))\n",
    "\n",
    "\n",
    "def get_basis_and_r(G, max_degree, compute_gradients=False):\n",
    "    \"\"\"Return equivariant weight basis (basis) and internodal distances (r).\n",
    "\n",
    "    Call this function *once* at the start of each forward pass of the model.\n",
    "    It computes the equivariant weight basis, W_J^lk(x), and internodal\n",
    "    distances, needed to compute varphi_J^lk(x), of eqn 8 of\n",
    "    https://arxiv.org/pdf/2006.10503.pdf. The return values of this function\n",
    "    can be shared as input across all SE(3)-Transformer layers in a model.\n",
    "\n",
    "    Args:\n",
    "        G: DGL graph instance of type dgl.DGLGraph()\n",
    "        max_degree: non-negative int for degree of highest feature-type\n",
    "        compute_gradients: controls whether to compute gradients during basis construction\n",
    "    Returns:\n",
    "        dict of equivariant bases, keys are in form '<d_in><d_out>'\n",
    "        vector of relative distances, ordered according to edge ordering of G\n",
    "    \"\"\"\n",
    "    basis = get_basis(G, max_degree, compute_gradients)\n",
    "    r = get_r(G)\n",
    "    return basis, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "be425371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26278/2732476931.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.file_address)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train-set, task: homo, source: ./QM9_data/QM9_data.pt, length: 100000\n",
      "MINIBATCH\n",
      "Graph(num_nodes=591, num_edges=10696,\n",
      "      ndata_schemes={'x': Scheme(shape=(3,), dtype=torch.float32), 'f': Scheme(shape=(6, 1), dtype=torch.float32)}\n",
      "      edata_schemes={'d': Scheme(shape=(3,), dtype=torch.float32), 'w': Scheme(shape=(5,), dtype=torch.float32)})\n",
      "torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vladygin/anaconda3/envs/torch_gcnn/lib/python3.9/site-packages/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "leg = {}\n",
    "\n",
    "dataset = QM9Dataset('./QM9_data/QM9_data.pt', \"homo\", mode='train', fully_connected=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "\n",
    "iter_dataloader = iter(dataloader) # so I can use next\n",
    "for i in range(1):\n",
    "    data = next(iter_dataloader)\n",
    "    print(\"MINIBATCH\")\n",
    "    print(data[0]) \n",
    "    print(data[1].shape) # batch size -> connected graph of size batch\n",
    "\n",
    "\n",
    "atom_feature_size = dataset.atom_feature_size\n",
    "num_degrees = 4\n",
    "num_channels = 32\n",
    "num_channels_out = num_channels*num_degrees\n",
    "edge_dim = dataset.num_bonds\n",
    "\n",
    "connection = 'skip'\n",
    "\n",
    "# building fibers for input data\n",
    "fibers = {'in': Fiber(1, atom_feature_size),\n",
    "           'mid': Fiber(num_degrees, num_channels),\n",
    "           'out': Fiber(1, num_channels_out)}\n",
    "\n",
    "\n",
    "f_in = fibers['in']\n",
    "f_out = fibers['out']\n",
    "# initialize dgl graph\n",
    "G = copy.deepcopy(dataset[0][0])\n",
    "\n",
    "basis, r = get_basis_and_r(G, num_degrees - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a3a6ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 6, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.ndata['f'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6773adff",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = {'0': G.ndata['f']} # type 0 node feature\n",
    "\n",
    "kernel_unary = {}\n",
    "# loop over (multiplicity, degree) tuples in input fiber\n",
    "for (mi, di) in f_in.structure:\n",
    "    # loop over (multiplicity, degree) tuples in output fiber\n",
    "    for (mo, do) in f_out.structure:\n",
    "        # generate a (mi * mo) unique kernels corresponding to every input\n",
    "        # and output channel pair\n",
    "        # store in dictionary with key f'({di},{do})'\n",
    "        kernel_unary[f'({di},{do})'] = PairwiseConv(di, mi, do, mo, edge_dim=edge_dim)\n",
    "        \n",
    "        \n",
    "# center -> center self connections\n",
    "# skip connection consideres output connection \n",
    "# as linear combination of input connections\n",
    "kernel_self = {\n",
    "    'skip': nn.ParameterDict(),\n",
    "    'TFN': nn.ParameterDict(),\n",
    "}\n",
    "\n",
    "# in constructor\n",
    "# skip connection consideres output connection \n",
    "# as linear combination of input connections\n",
    "for mi, di in f_in.structure:\n",
    "    # proceed if input type is also an output type\n",
    "    if di in f_out.degrees:\n",
    "        # extract num of output channels of the type\n",
    "        mo = f_out.structure_dict[di]\n",
    "        # initialize learnable mi x mo weight matrix with random integers and scale down\n",
    "        # singleton dimension used to broadcast across nodes\n",
    "        W = nn.Parameter(torch.randn(1, mo, mi)/np.sqrt(mi))\n",
    "        kernel_self['skip'][f'{di}'] = W\n",
    "        \n",
    "        \n",
    "# in constructor\n",
    "# mixing connection consideres output self features\n",
    "# as linear combination of output connections\n",
    "for mo, do in f_in.structure:\n",
    "    # initialize square learnable weight matrix of random integers\n",
    "    # and scale down\n",
    "    W = nn.Parameter(torch.randn(1, mo, mo) / np.sqrt(mo))\n",
    "    kernel_self['TFN'][f\"{do}\"] = W\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df6f9c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[ 3.4800e-02,  1.3604e+00,  1.5080e-01],\n",
       "        [-9.5000e-03, -1.6100e-02,  5.3400e-02],\n",
       "        [-1.1400e+00, -7.4810e-01,  2.6900e-02],\n",
       "        [-1.1403e+00, -2.1319e+00, -1.2000e-02],\n",
       "        [ 3.7300e-02, -2.7655e+00, -1.5400e-02],\n",
       "        [-1.0070e-01, -4.0993e+00, -4.1100e-02],\n",
       "        [ 1.2330e+00, -2.1386e+00,  6.0000e-04],\n",
       "        [ 1.3055e+00, -6.9670e-01,  1.7000e-02],\n",
       "        [ 2.3546e+00,  5.4000e-02, -7.2000e-03],\n",
       "        [ 9.6010e-01,  1.7065e+00, -7.8600e-02],\n",
       "        [-7.2620e-01,  1.8749e+00, -2.6370e-01],\n",
       "        [-2.1488e+00, -3.6290e-01,  4.1400e-02],\n",
       "        [-1.0447e+00, -4.3020e+00, -4.4600e-02],\n",
       "        [ 2.1340e+00, -2.7379e+00, -1.4600e-02],\n",
       "        [ 3.2064e+00, -5.0600e-01, -4.0100e-02]]), 'f': tensor([[[0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [7.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [6.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [6.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [8.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [6.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [8.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [6.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [6.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [7.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]]])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.ndata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77bf948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with G.local_scope():\n",
    "    # Add node features to local graph scope\n",
    "    for k, v in h.items():\n",
    "        G.ndata[k] = v\n",
    "\n",
    "    # Add edge features\n",
    "    if 'w' in G.edata.keys():\n",
    "        w = G.edata['w']\n",
    "        feat = torch.cat([w, r], -1)\n",
    "    else:\n",
    "        feat = torch.cat([r,], -1)\n",
    "\n",
    "    for (mi, di) in f_in.structure:\n",
    "        for (mo, do) in f_out.structure:\n",
    "            etype = f\"({di},{do})\"\n",
    "            G.edata[etype] = kernel_unary[etype](feat, basis) \n",
    "\n",
    "    # defining the function for reduction\n",
    "    # function is defined for each output feature type d\n",
    "    for do in f_out.degrees:\n",
    "\n",
    "        # edge user-defined function in DGL that computes the message for a single output feature type\n",
    "        # do\n",
    "        def udf_u_mul_e(do):\n",
    "            # calculate neighbor -> center message for type single output\n",
    "            # feature type do\n",
    "            def fnc(edges):\n",
    "                msg = 0\n",
    "                for mi, di in f_in.structure:\n",
    "                    # extract all feature channels of type di from the neighborhood\n",
    "                    # nodes and condense into single vector\n",
    "                    # src has shape (edges, mi*(2*di + 1), 1)\n",
    "                    src = edges.src[f'{di}'].view(-1, mi*(2*di + 1), 1)\n",
    "\n",
    "                    # extract kernel for input type di and output type do\n",
    "                    edge = edges.data[f\"({di},{do})\"]\n",
    "                    # matrix multiplication to get (mo*(2*do + 1))-dimentional vector and add to total msg\n",
    "                    msg += msg + torch.matmul(edge, src)\n",
    "\n",
    "                # reshape message to separate output channels\n",
    "                msg = msg.view(msg.shape[0], -1, 2*do + 1)\n",
    "\n",
    "\n",
    "                # center -> center message\n",
    "                if connection == 'skip':\n",
    "                    # extract all input features of type do from all nodes\n",
    "                    dst = edges.dst[f'{do}']\n",
    "                    # extract self-intecation weights for type do channels\n",
    "                    W = kernel_self[connection][f'{do}']\n",
    "                    # calculate the array of self-interaction tensors with shape (nodes, mo, 2*do +1)\n",
    "                    self_int = torch.matmul(W, dst)\n",
    "                    msg = msg + self_int\n",
    "\n",
    "                # in user-defined DGL edge -> node function\n",
    "                # extract weight array of shape (1, mo, mo)\n",
    "                if connection == 'TFN':\n",
    "                    W = kernel_self['TFN'][f'{do}']\n",
    "                    # matrix multiplication that generates output feature tensor for \n",
    "                    # for degree do\n",
    "                    msg = torch.matmul(W, msg)\n",
    "\n",
    "                return {'msg': msg.view(msg.shape[0], -1, 2*do + 1)}\n",
    "            return fnc\n",
    "\n",
    "        # call update all function that takes (message_func, reduce_func) as input\n",
    "        G.update_all(udf_u_mul_e(do), fn.mean('msg', f'out{do}'))\n",
    "\n",
    "    # return a dictionary of the output node features where every degree\n",
    "    # is linked to an array with shape (edges, mo, 2*do + 1) by extracting\n",
    "    # the node data stored from calling update_all\n",
    "\n",
    "    f_mid = {f'{do}': G.ndata[f'out{do}'] for do in f_out.degrees}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f79de794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['x', 'f']), dict_keys(['0']))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.ndata.keys(), f_mid.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9b074fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mid_my = f_mid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c045a439-0204-4ad4-8c69-86013897acd1",
   "metadata": {},
   "source": [
    "### Reference model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "419ba34e-53b3-4925-9e60-328b1d8366ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GConvSE3(nn.Module):\n",
    "    \"\"\"A tensor field network layer as a DGL module.\n",
    "\n",
    "    GConvSE3 stands for a Graph Convolution SE(3)-equivariant layer. It is the\n",
    "    equivalent of a linear layer in an MLP, a conv layer in a CNN, or a graph\n",
    "    conv layer in a GCN.\n",
    "\n",
    "    At each node, the activations are split into different \"feature types\",\n",
    "    indexed by the SE(3) representation type: non-negative integers 0, 1, 2, ..\n",
    "    \"\"\"\n",
    "    def __init__(self, f_in, f_out, self_interaction: bool=False, edge_dim: int=0, flavor='skip'):\n",
    "        \"\"\"SE(3)-equivariant Graph Conv Layer\n",
    "\n",
    "        Args:\n",
    "            f_in: list of tuples [(multiplicities, type),...]\n",
    "            f_out: list of tuples [(multiplicities, type),...]\n",
    "            self_interaction: include self-interaction in convolution\n",
    "            edge_dim: number of dimensions for edge embedding\n",
    "            flavor: allows ['TFN', 'skip'], where 'skip' adds a skip connection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        self.edge_dim = edge_dim\n",
    "        self.self_interaction = self_interaction\n",
    "        self.flavor = flavor\n",
    "\n",
    "        # Neighbor -> center weights\n",
    "        self.kernel_unary = nn.ModuleDict()\n",
    "        for (mi, di) in self.f_in.structure:\n",
    "            for (mo, do) in self.f_out.structure:\n",
    "                self.kernel_unary[f'({di},{do})'] = PairwiseConv(di, mi, do, mo, edge_dim=edge_dim)\n",
    "\n",
    "        # Center -> center weights\n",
    "        self.kernel_self = nn.ParameterDict()\n",
    "        if self_interaction:\n",
    "            assert self.flavor in ['TFN', 'skip']\n",
    "            if self.flavor == 'TFN':\n",
    "                for m_out, d_out in self.f_out.structure:\n",
    "                    W = nn.Parameter(torch.randn(1, m_out, m_out) / np.sqrt(m_out))\n",
    "                    self.kernel_self[f'{d_out}'] = W\n",
    "            elif self.flavor == 'skip':\n",
    "                for m_in, d_in in self.f_in.structure:\n",
    "                    if d_in in self.f_out.degrees:\n",
    "                        m_out = self.f_out.structure_dict[d_in]\n",
    "                        W = nn.Parameter(torch.randn(1, m_out, m_in) / np.sqrt(m_in))\n",
    "                        self.kernel_self[f'{d_in}'] = W\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'GConvSE3(structure={self.f_out}, self_interaction={self.self_interaction})'\n",
    "\n",
    "\n",
    "    def udf_u_mul_e(self, d_out):\n",
    "        \"\"\"Compute the convolution for a single output feature type.\n",
    "\n",
    "        This function is set up as a User Defined Function in DGL.\n",
    "\n",
    "        Args:\n",
    "            d_out: output feature type\n",
    "        Returns:\n",
    "            edge -> node function handle\n",
    "        \"\"\"\n",
    "        def fnc(edges):\n",
    "            # Neighbor -> center messages\n",
    "            msg = 0\n",
    "            for m_in, d_in in self.f_in.structure:\n",
    "                src = edges.src[f'{d_in}'].view(-1, m_in*(2*d_in+1), 1)\n",
    "                edge = edges.data[f'({d_in},{d_out})']\n",
    "                msg = msg + torch.matmul(edge, src)\n",
    "            msg = msg.view(msg.shape[0], -1, 2*d_out+1)\n",
    "\n",
    "            # Center -> center messages\n",
    "            if self.self_interaction:\n",
    "                if f'{d_out}' in self.kernel_self.keys():\n",
    "                    if self.flavor == 'TFN':\n",
    "                        W = self.kernel_self[f'{d_out}']\n",
    "                        msg = torch.matmul(W, msg)\n",
    "                    if self.flavor == 'skip':\n",
    "                        dst = edges.dst[f'{d_out}']\n",
    "                        W = self.kernel_self[f'{d_out}']\n",
    "                        msg = msg + torch.matmul(W, dst)\n",
    "\n",
    "            return {'msg': msg.view(msg.shape[0], -1, 2*d_out+1)}\n",
    "        return fnc\n",
    "\n",
    "    @profile\n",
    "    def forward(self, h, G=None, r=None, basis=None, **kwargs):\n",
    "        \"\"\"Forward pass of the linear layer\n",
    "\n",
    "        Args:\n",
    "            G: minibatch of (homo)graphs\n",
    "            h: dict of features\n",
    "            r: inter-atomic distances\n",
    "            basis: pre-computed Q * Y\n",
    "        Returns:\n",
    "            tensor with new features [B, n_points, n_features_out]\n",
    "        \"\"\"\n",
    "        with G.local_scope():\n",
    "            # Add node features to local graph scope\n",
    "            for k, v in h.items():\n",
    "                G.ndata[k] = v\n",
    "\n",
    "            # Add edge features\n",
    "            if 'w' in G.edata.keys():\n",
    "                w = G.edata['w']\n",
    "                feat = torch.cat([w, r], -1)\n",
    "            else:\n",
    "                feat = torch.cat([r, ], -1)\n",
    "\n",
    "            for (mi, di) in self.f_in.structure:\n",
    "                for (mo, do) in self.f_out.structure:\n",
    "                    etype = f'({di},{do})'\n",
    "                    G.edata[etype] = self.kernel_unary[etype](feat, basis)\n",
    "\n",
    "            # Perform message-passing for each output feature type\n",
    "            for d in self.f_out.degrees:\n",
    "                G.update_all(self.udf_u_mul_e(d), fn.mean('msg', f'out{d}'))\n",
    "\n",
    "            return {f'{d}': G.ndata[f'out{d}'] for d in self.f_out.degrees}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f232813b-3c45-4935-9cec-8dd855f9318e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26278/2732476931.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.file_address)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train-set, task: homo, source: ./QM9_data/QM9_data.pt, length: 100000\n",
      "MINIBATCH\n",
      "Graph(num_nodes=591, num_edges=10696,\n",
      "      ndata_schemes={'x': Scheme(shape=(3,), dtype=torch.float32), 'f': Scheme(shape=(6, 1), dtype=torch.float32)}\n",
      "      edata_schemes={'d': Scheme(shape=(3,), dtype=torch.float32), 'w': Scheme(shape=(5,), dtype=torch.float32)})\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "leg = {}\n",
    "\n",
    "dataset = QM9Dataset('./QM9_data/QM9_data.pt', \"homo\", mode='train', fully_connected=True)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate)\n",
    "\n",
    "iter_dataloader = iter(dataloader) # so I can use next\n",
    "for i in range(1):\n",
    "    data = next(iter_dataloader)\n",
    "    print(\"MINIBATCH\")\n",
    "    print(data[0]) \n",
    "    print(data[1].shape) # batch size -> connected graph of size batch\n",
    "\n",
    "\n",
    "atom_feature_size = dataset.atom_feature_size\n",
    "num_degrees = 4\n",
    "num_channels = 32\n",
    "num_channels_out = num_channels*num_degrees\n",
    "edge_dim = dataset.num_bonds\n",
    "\n",
    "connection = 'skip'\n",
    "\n",
    "# building fibers for input data\n",
    "fibers = {'in': Fiber(1, atom_feature_size),\n",
    "           'mid': Fiber(num_degrees, num_channels),\n",
    "           'out': Fiber(1, num_channels_out)}\n",
    "\n",
    "\n",
    "f_in = fibers['in']\n",
    "f_out = fibers['out']\n",
    "# initialize dgl graph\n",
    "G = copy.deepcopy(dataset[0][0])\n",
    "\n",
    "\n",
    "h = {'0': G.ndata['f']} # type 0 node feature\n",
    "basis, r = get_basis_and_r(G, num_degrees - 1)\n",
    "\n",
    "layer = GConvSE3(f_in = f_in, \n",
    "                 f_out = f_out, \n",
    "                 edge_dim=edge_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9be228c5-4a76-4398-9f1a-aad6bdc3b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mid_new = layer(h, G = G, r = r, basis = basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80f7dd95-a91a-4d24-88e6-1403aa58da60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 128, 1]), tensor(216.5136, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mid_new['0'].shape, f_mid_new['0'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "412b21b7-10c8-464e-adfe-d2143ba869f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 128, 1]), tensor(380.8578, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mid_my['0'].shape, f_mid_my['0'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "32de5c14-cd83-480a-8cf6-746c4f9dc0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU test\n",
    "\n",
    "G_cu = G.to(device)\n",
    "\n",
    "h_cu = {'0': G_cu.ndata['f']} # type 0 node feature\n",
    "\n",
    "layer_cu = GConvSE3(f_in = f_in, \n",
    "                 f_out = f_out, \n",
    "                 edge_dim=edge_dim).to(device)\n",
    "\n",
    "basis_cu = {key: basis[key].to(device) for key in basis}\n",
    "r_cu = r.to(device)\n",
    "f_mid_new_cu = layer_cu(h, G = G_cu, r = r_cu, basis = basis_cu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db4ced2d-021c-46dd-b301-a6ca0a2ad97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-219.1493, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_mid_new_cu['0'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af964ddd-4c30-495b-8101-b0c8b41b5912",
   "metadata": {},
   "source": [
    "## SE3 transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5a2ae-da4a-4132-b3c0-2d9ded81eee8",
   "metadata": {},
   "source": [
    "### Query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d9884bd-22c0-4caa-8101-48cc6f99866c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(128, 0)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First define the fiber f_mid_in with same multiplicities as value msgs with\n",
    "# structure f_mid_out, \n",
    "# !!! but keep only degrees in input f_in\n",
    "\n",
    "# building fibers for input data\n",
    "fibers = {'in': Fiber(1, atom_feature_size),\n",
    "           'mid': Fiber(num_degrees, num_channels),\n",
    "           'out': Fiber(1, num_channels_out)}\n",
    "\n",
    "\n",
    "f_in = fibers['in']\n",
    "f_out = fibers['out']\n",
    "\n",
    "f_mid_out = copy.deepcopy(f_out).structure_dict\n",
    "\n",
    "f_mid_in = Fiber(dictionary={d: m for d, m in f_mid_out.items() if d in f_in.degrees})\n",
    "\n",
    "f_mid_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9c77ae56-b4a2-41f5-bf2b-83a9daa19517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing weights\n",
    "transform = nn.ParameterDict()\n",
    "# loop through all degrees in f_mid_in\n",
    "for m_mid, d_mid in f_mid_in.structure:\n",
    "    # extract number of input channels of degree d_mid to define\n",
    "    # dimensions of weight matrix\n",
    "    mi = f_in.structure_dict[d_mid]\n",
    "    # initialize m_mid x mi weight matrix with random integers and scale down\n",
    "    transform[str(d_mid)] = nn.Parameter(torch.randn(m_mid, mi) / np.sqrt(mi), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1f0634af-da1d-4c8e-b45d-274d30e62044",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = {'0': G.ndata['f']} # type 0 node feature\n",
    "\n",
    "with G.local_scope():\n",
    "    # Add node features to local graph scope\n",
    "    for k, v in h.items():\n",
    "        G.ndata[k] = v\n",
    "\n",
    "    # Add edge features\n",
    "    if 'w' in G.edata.keys():\n",
    "        w = G.edata['w']\n",
    "        feat = torch.cat([w, r], -1)\n",
    "    else:\n",
    "        feat = torch.cat([r, ], -1)\n",
    "\n",
    "    # Perfoming linear self attention transformation\n",
    "    output = {}\n",
    "    # loop through output degrees in features dictionary and extract\n",
    "    # features f\n",
    "    for do, f in h.items():\n",
    "        # check if there is a query matrix corresponding to the degree do\n",
    "        if str(do) in transform.keys():\n",
    "            # calculate the query for every channel of degree do\n",
    "            # output has shape (mo, 2*do + 1)\n",
    "            output[do] = torch.matmul(transform[str(do)], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "557b441b-2fd6-46ee-b0a7-230e287c30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref from the code\n",
    "class G1x1SE3(nn.Module):\n",
    "    \"\"\"Graph Linear SE(3)-equivariant layer, equivalent to a 1x1 convolution.\n",
    "\n",
    "    This is equivalent to a self-interaction layer in TensorField Networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, f_in, f_out, learnable=True):\n",
    "        \"\"\"SE(3)-equivariant 1x1 convolution.\n",
    "\n",
    "        Args:\n",
    "            f_in: input Fiber() of feature multiplicities and types\n",
    "            f_out: output Fiber() of feature multiplicities and types\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "\n",
    "        # Linear mappings: 1 per output feature type\n",
    "        self.transform = nn.ParameterDict()\n",
    "        for m_out, d_out in self.f_out.structure:\n",
    "            m_in = self.f_in.structure_dict[d_out]\n",
    "            self.transform[str(d_out)] = nn.Parameter(torch.randn(m_out, m_in) / np.sqrt(m_in), requires_grad=learnable)\n",
    "\n",
    "    def __repr__(self):\n",
    "         return f\"G1x1SE3(structure={self.f_out})\"\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        output = {}\n",
    "        for k, v in features.items():\n",
    "            if str(k) in self.transform.keys():\n",
    "                output[k] = torch.matmul(self.transform[str(k)], v)\n",
    "        return output\n",
    "\n",
    "# initialize the function for generating the query that projects from f_in to f_mid_in\n",
    "GMAB = {}\n",
    "GMAB['q'] = G1x1SE3(f_in, f_mid_in)\n",
    "\n",
    "q = GMAB['q'](h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58009d3c-9f8b-4a30-ab7d-0447b5b28eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation along channels and moments of features\n",
    "# F: list where each element is an array with shape (m, 2*d + 1) for each feature degree d\n",
    "# assume for now that H = 1 (single attention head)\n",
    "def fiber2head(F, H, structure):\n",
    "    # squeeze each array in the list into a m * (2*d + 1)-dimentional vector \n",
    "    # of all channels concatenated along the last dimention\n",
    "    fibers = [F[f'{d}'].view(*F[f'{d}'].shape[:-2], H, -1) for d in structure.degrees]\n",
    "    # concatenate across the last dimension of every array in the list to get a single vector\n",
    "    fibers = torch.cat(fibers, -1)\n",
    "    return fibers\n",
    "\n",
    "n_heads = 1\n",
    "\n",
    "G.ndata['q'] = fiber2head(q, 1, f_mid_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ba8ab86-2707-4f40-ba09-11d2a423f70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 128])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.ndata['q'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3a8f65-df37-4fcc-9572-d5c189d3ea76",
   "metadata": {},
   "source": [
    "### Key embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8e08c831-2021-4ebf-a071-d737c5508fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys are same as TFN conv layers but no self interaction and head attention dim is added to comp\n",
    "class GConvSE3Partial(nn.Module):\n",
    "    \"\"\"Graph SE(3)-equivariant node -> edge layer\"\"\"\n",
    "    def __init__(self, f_in, f_out, edge_dim: int=0, x_ij=None):\n",
    "        \"\"\"SE(3)-equivariant partial convolution.\n",
    "\n",
    "        A partial convolution computes the inner product between a kernel and\n",
    "        each input channel, without summing over the result from each input\n",
    "        channel. This unfolded structure makes it amenable to be used for\n",
    "        computing the value-embeddings of the attention mechanism.\n",
    "\n",
    "        Args:\n",
    "            f_in: list of tuples [(multiplicities, type),...]\n",
    "            f_out: list of tuples [(multiplicities, type),...]\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.f_out = f_out\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        # adding/concatinating relative position to feature vectors\n",
    "        # 'cat' concatenates relative position & existing feature vector\n",
    "        # 'add' adds it, but only if multiplicity > 1\n",
    "        assert x_ij in [None, 'cat', 'add']\n",
    "        self.x_ij = x_ij\n",
    "        if x_ij == 'cat':\n",
    "            self.f_in = Fiber.combine(f_in, Fiber(structure=[(1,1)]))\n",
    "        else:\n",
    "            self.f_in = f_in\n",
    "\n",
    "        # Node -> edge weights\n",
    "        self.kernel_unary = nn.ModuleDict()\n",
    "        for (mi, di) in self.f_in.structure:\n",
    "            for (mo, do) in self.f_out.structure:\n",
    "                self.kernel_unary[f'({di},{do})'] = PairwiseConv(di, mi, do, mo, edge_dim=edge_dim)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'GConvSE3Partial(structure={self.f_out})'\n",
    "\n",
    "    def udf_u_mul_e(self, d_out):\n",
    "        \"\"\"Compute the partial convolution for a single output feature type.\n",
    "\n",
    "        This function is set up as a User Defined Function in DGL.\n",
    "\n",
    "        Args:\n",
    "            d_out: output feature type\n",
    "        Returns:\n",
    "            node -> edge function handle\n",
    "        \"\"\"\n",
    "        def fnc(edges):\n",
    "            # Neighbor -> center messages\n",
    "            msg = 0\n",
    "            for m_in, d_in in self.f_in.structure:\n",
    "                # if type 1 and flag set, add relative position as feature\n",
    "                if self.x_ij == 'cat' and d_in == 1:\n",
    "                    # relative positions\n",
    "                    rel = (edges.dst['x'] - edges.src['x']).view(-1, 3, 1)\n",
    "                    m_ori = m_in - 1\n",
    "                    if m_ori == 0:\n",
    "                        # no type 1 input feature, just use relative position\n",
    "                        src = rel\n",
    "                    else:\n",
    "                        # features of src node, shape [edges, m_in*(2l+1), 1]\n",
    "                        src = edges.src[f'{d_in}'].view(-1, m_ori*(2*d_in+1), 1)\n",
    "                        # add to feature vector\n",
    "                        src = torch.cat([src, rel], dim=1)\n",
    "                elif self.x_ij == 'add' and d_in == 1 and m_in > 1:\n",
    "                    src = edges.src[f'{d_in}'].view(-1, m_in*(2*d_in+1), 1)\n",
    "                    rel = (edges.dst['x'] - edges.src['x']).view(-1, 3, 1)\n",
    "                    src[..., :3, :1] = src[..., :3, :1] + rel\n",
    "                else:\n",
    "                    src = edges.src[f'{d_in}'].view(-1, m_in*(2*d_in+1), 1)\n",
    "                edge = edges.data[f'({d_in},{d_out})']\n",
    "                msg = msg + torch.matmul(edge, src)\n",
    "            msg = msg.view(msg.shape[0], -1, 2*d_out+1)\n",
    "\n",
    "            return {f'out{d_out}': msg.view(msg.shape[0], -1, 2*d_out+1)}\n",
    "        return fnc\n",
    "\n",
    "    @profile\n",
    "    def forward(self, h, G=None, r=None, basis=None, **kwargs):\n",
    "        \"\"\"Forward pass of the linear layer\n",
    "\n",
    "        Args:\n",
    "            h: dict of node-features\n",
    "            G: minibatch of (homo)graphs\n",
    "            r: inter-atomic distances\n",
    "            basis: pre-computed Q * Y\n",
    "        Returns:\n",
    "            tensor with new features [B, n_points, n_features_out]\n",
    "        \"\"\"\n",
    "        with G.local_scope():\n",
    "            # Add node features to local graph scope\n",
    "            for k, v in h.items():\n",
    "                G.ndata[k] = v\n",
    "\n",
    "            # Add edge features\n",
    "            if 'w' in G.edata.keys():\n",
    "                w = G.edata['w'] # shape: [#edges_in_batch, #bond_types]\n",
    "                feat = torch.cat([w, r], -1)\n",
    "            else:\n",
    "                feat = torch.cat([r, ], -1)\n",
    "            for (mi, di) in self.f_in.structure:\n",
    "                for (mo, do) in self.f_out.structure:\n",
    "                    etype = f'({di},{do})'\n",
    "                    G.edata[etype] = self.kernel_unary[etype](feat, basis)\n",
    "\n",
    "            # Perform message-passing for each output feature type\n",
    "            for d in self.f_out.degrees:\n",
    "                G.apply_edges(self.udf_u_mul_e(d))\n",
    "\n",
    "            return {f'{d}': G.edata[f'out{d}'] for d in self.f_out.degrees}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2630e701-b2cd-4057-894d-9b0f3d4766eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge dim : used to determine the dim of input (edge_dim + 1) to the radial function\n",
    "# if only radial distance, the edge_dim is 0\n",
    "# x_ij: type-1 displacement vector used as edge feature (more on this later)\n",
    "GMAB['k'] = GConvSE3Partial(f_in, f_mid_in, edge_dim=edge_dim, x_ij = 'cat')\n",
    "\n",
    "k = GMAB['k'](h, G = G, r = r, basis = basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "980d9f6d-23b6-47d9-8b93-6aa5bf2d81e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([210, 1, 128])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the result to edge feature\n",
    "G.edata['k'] = fiber2head(k, n_heads, f_mid_in)\n",
    "\n",
    "G.edata['k'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2f3a6-5a35-485e-bd28-d398ddcd1b66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gcnn",
   "language": "python",
   "name": "torch_gcnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
